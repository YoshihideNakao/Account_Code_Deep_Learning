{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_file = open(\"Data/勘定科目コード.csv\",'r')\n",
    "account_data = account_file.readlines()\n",
    "account_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込んだデータをPythonリスト形式に変換して、先頭のラベルと2桁目以降の画像データを分離する\n",
    "n = len(account_data)\n",
    "account_list = []\n",
    "account_target = []\n",
    "for i in range(n):\n",
    "    account_list.append(list(map(int, account_data[i].split(',')[0])))\n",
    "    account_target.append(int(account_data[i].split(',')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用にデータを抽出する\n",
    "#N = 30000 # MNISTの一部のデータで実験\n",
    "# indices = np.random.permutation(range(n))[:N]\n",
    "#X = []\n",
    "#y = []\n",
    "#for i in indices:\n",
    "#    X.append(mnist_list[i])\n",
    "#    y.append(mnist_target[i])\n",
    "\n",
    "X = account_list\n",
    "y = account_target\n",
    "Y = np.eye(5)[y] # 1-of-k表現に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの正規化\n",
    "X = np.array(X)\n",
    "X = X / X.max()\n",
    "X = X - X.mean(axis=1).reshape(len(X), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "N_train = 0.8\n",
    "N_validation =0.2\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=N_train)\n",
    "\n",
    "# 訓練データをさらに訓練データと検証データに分類\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size = N_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 445\n",
      "Trainable params: 385\n",
      "Non-trainable params: 60\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# モデル設定\n",
    "n_in = len(X[0])    # 入力層のノード数\n",
    "n_hiddens = [10,10,10]   # 隠れ層のノード数\n",
    "n_out = len(Y[0])   # 出力層のノード数\n",
    "activation = 'relu' # 活性化関数　出力の値を0~1の確率変数に置き換える 'sigmoid''tanh''relu'　\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    return np.sqrt(2.0 / shape[0]) * np.random.normal(size=shape)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 入力層、隠れ層を生成する\n",
    "for i, input_dim in enumerate(([n_in] + n_hiddens)[:-1]):\n",
    "    model.add(Dense(n_hiddens[i], input_dim=input_dim,\n",
    "                    kernel_initializer=weight_variable))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.5)) # オーバーフィッティング対策　学習時に間引くノードの確率\n",
    "\n",
    "# 出力層を生成する  \n",
    "model.add(Dense(n_out, kernel_initializer=weight_variable))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 学習の準備　optimizer:勾配法　loss:誤差関数　metrics:評価関数\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.01, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 116 samples, validate on 30 samples\n",
      "Epoch 1/1000\n",
      "116/116 [==============================] - 1s 13ms/step - loss: 2.1097 - acc: 0.2069 - val_loss: 1.5273 - val_acc: 0.3000\n",
      "Epoch 2/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 2.2810 - acc: 0.2155 - val_loss: 1.5220 - val_acc: 0.3333\n",
      "Epoch 3/1000\n",
      "116/116 [==============================] - 0s 49us/step - loss: 2.2210 - acc: 0.2155 - val_loss: 1.5146 - val_acc: 0.3333\n",
      "Epoch 4/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.6093 - acc: 0.3276 - val_loss: 1.5060 - val_acc: 0.3667\n",
      "Epoch 5/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 2.0497 - acc: 0.2500 - val_loss: 1.5000 - val_acc: 0.4333\n",
      "Epoch 6/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.8578 - acc: 0.2759 - val_loss: 1.4949 - val_acc: 0.4667\n",
      "Epoch 7/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.8298 - acc: 0.2931 - val_loss: 1.4895 - val_acc: 0.4667\n",
      "Epoch 8/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.8584 - acc: 0.2759 - val_loss: 1.4855 - val_acc: 0.5000\n",
      "Epoch 9/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.6489 - acc: 0.3966 - val_loss: 1.4814 - val_acc: 0.5333\n",
      "Epoch 10/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.7454 - acc: 0.3534 - val_loss: 1.4764 - val_acc: 0.5333\n",
      "Epoch 11/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.7298 - acc: 0.4052 - val_loss: 1.4737 - val_acc: 0.5667\n",
      "Epoch 12/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.5179 - acc: 0.4483 - val_loss: 1.4702 - val_acc: 0.5333\n",
      "Epoch 13/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.5000 - acc: 0.4397 - val_loss: 1.4664 - val_acc: 0.4333\n",
      "Epoch 14/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.4632 - acc: 0.4655 - val_loss: 1.4628 - val_acc: 0.4333\n",
      "Epoch 15/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.4300 - acc: 0.4483 - val_loss: 1.4588 - val_acc: 0.4333\n",
      "Epoch 16/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.5095 - acc: 0.4655 - val_loss: 1.4550 - val_acc: 0.4333\n",
      "Epoch 17/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.4251 - acc: 0.4914 - val_loss: 1.4512 - val_acc: 0.4333\n",
      "Epoch 18/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.3913 - acc: 0.5431 - val_loss: 1.4472 - val_acc: 0.4333\n",
      "Epoch 19/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.4095 - acc: 0.5345 - val_loss: 1.4445 - val_acc: 0.4333\n",
      "Epoch 20/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.2431 - acc: 0.5690 - val_loss: 1.4414 - val_acc: 0.4333\n",
      "Epoch 21/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.3852 - acc: 0.5431 - val_loss: 1.4391 - val_acc: 0.4333\n",
      "Epoch 22/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.3494 - acc: 0.5948 - val_loss: 1.4382 - val_acc: 0.4333\n",
      "Epoch 23/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.3421 - acc: 0.5259 - val_loss: 1.4355 - val_acc: 0.4333\n",
      "Epoch 24/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.2948 - acc: 0.5690 - val_loss: 1.4341 - val_acc: 0.4333\n",
      "Epoch 25/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.3488 - acc: 0.5603 - val_loss: 1.4325 - val_acc: 0.4333\n",
      "Epoch 26/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.2367 - acc: 0.5603 - val_loss: 1.4309 - val_acc: 0.4333\n",
      "Epoch 27/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.3593 - acc: 0.5517 - val_loss: 1.4289 - val_acc: 0.4333\n",
      "Epoch 28/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1830 - acc: 0.5776 - val_loss: 1.4285 - val_acc: 0.4333\n",
      "Epoch 29/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.3260 - acc: 0.5259 - val_loss: 1.4263 - val_acc: 0.4333\n",
      "Epoch 30/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.4210 - acc: 0.5345 - val_loss: 1.4236 - val_acc: 0.4333\n",
      "Epoch 31/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.2764 - acc: 0.5431 - val_loss: 1.4199 - val_acc: 0.4333\n",
      "Epoch 32/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.3243 - acc: 0.5431 - val_loss: 1.4155 - val_acc: 0.4333\n",
      "Epoch 33/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.2641 - acc: 0.5603 - val_loss: 1.4093 - val_acc: 0.4333\n",
      "Epoch 34/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.1572 - acc: 0.5603 - val_loss: 1.4034 - val_acc: 0.4333\n",
      "Epoch 35/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.2533 - acc: 0.5517 - val_loss: 1.3981 - val_acc: 0.4333\n",
      "Epoch 36/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1699 - acc: 0.5603 - val_loss: 1.3923 - val_acc: 0.4333\n",
      "Epoch 37/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.2649 - acc: 0.5862 - val_loss: 1.3862 - val_acc: 0.4333\n",
      "Epoch 38/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.2257 - acc: 0.5690 - val_loss: 1.3793 - val_acc: 0.4333\n",
      "Epoch 39/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.3094 - acc: 0.5172 - val_loss: 1.3731 - val_acc: 0.4333\n",
      "Epoch 40/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.1959 - acc: 0.5690 - val_loss: 1.3666 - val_acc: 0.4333\n",
      "Epoch 41/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1848 - acc: 0.5690 - val_loss: 1.3603 - val_acc: 0.4333\n",
      "Epoch 42/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.2445 - acc: 0.5603 - val_loss: 1.3545 - val_acc: 0.4333\n",
      "Epoch 43/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.2013 - acc: 0.5517 - val_loss: 1.3488 - val_acc: 0.4333\n",
      "Epoch 44/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1624 - acc: 0.5603 - val_loss: 1.3436 - val_acc: 0.4333\n",
      "Epoch 45/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.2287 - acc: 0.5603 - val_loss: 1.3394 - val_acc: 0.4333\n",
      "Epoch 46/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.2118 - acc: 0.5603 - val_loss: 1.3353 - val_acc: 0.4333\n",
      "Epoch 47/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.2247 - acc: 0.5517 - val_loss: 1.3321 - val_acc: 0.4333\n",
      "Epoch 48/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1981 - acc: 0.5345 - val_loss: 1.3290 - val_acc: 0.4333\n",
      "Epoch 49/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0938 - acc: 0.5862 - val_loss: 1.3252 - val_acc: 0.4333\n",
      "Epoch 50/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.1634 - acc: 0.5517 - val_loss: 1.3207 - val_acc: 0.4333\n",
      "Epoch 51/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1797 - acc: 0.5603 - val_loss: 1.3164 - val_acc: 0.4333\n",
      "Epoch 52/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.1653 - acc: 0.5690 - val_loss: 1.3112 - val_acc: 0.4333\n",
      "Epoch 53/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1247 - acc: 0.5431 - val_loss: 1.3062 - val_acc: 0.4333\n",
      "Epoch 54/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.1808 - acc: 0.5603 - val_loss: 1.3015 - val_acc: 0.4333\n",
      "Epoch 55/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1334 - acc: 0.5603 - val_loss: 1.2962 - val_acc: 0.4333\n",
      "Epoch 56/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.1331 - acc: 0.5603 - val_loss: 1.2907 - val_acc: 0.4333\n",
      "Epoch 57/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1101 - acc: 0.5862 - val_loss: 1.2847 - val_acc: 0.4333\n",
      "Epoch 58/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0404 - acc: 0.5690 - val_loss: 1.2786 - val_acc: 0.4333\n",
      "Epoch 59/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1543 - acc: 0.5776 - val_loss: 1.2723 - val_acc: 0.4333\n",
      "Epoch 60/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0979 - acc: 0.5690 - val_loss: 1.2660 - val_acc: 0.4333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1256 - acc: 0.5690 - val_loss: 1.2593 - val_acc: 0.4333\n",
      "Epoch 62/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.1030 - acc: 0.5690 - val_loss: 1.2534 - val_acc: 0.4333\n",
      "Epoch 63/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0858 - acc: 0.6034 - val_loss: 1.2469 - val_acc: 0.4333\n",
      "Epoch 64/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.1704 - acc: 0.5431 - val_loss: 1.2412 - val_acc: 0.4333\n",
      "Epoch 65/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0635 - acc: 0.5776 - val_loss: 1.2349 - val_acc: 0.4333\n",
      "Epoch 66/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.1038 - acc: 0.5690 - val_loss: 1.2275 - val_acc: 0.4333\n",
      "Epoch 67/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1569 - acc: 0.5690 - val_loss: 1.2207 - val_acc: 0.4333\n",
      "Epoch 68/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1662 - acc: 0.5603 - val_loss: 1.2137 - val_acc: 0.4333\n",
      "Epoch 69/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.1277 - acc: 0.5690 - val_loss: 1.2067 - val_acc: 0.4333\n",
      "Epoch 70/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1330 - acc: 0.5690 - val_loss: 1.2007 - val_acc: 0.4333\n",
      "Epoch 71/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1205 - acc: 0.5517 - val_loss: 1.1945 - val_acc: 0.4333\n",
      "Epoch 72/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 1.0960 - acc: 0.5603 - val_loss: 1.1886 - val_acc: 0.4333\n",
      "Epoch 73/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0682 - acc: 0.6034 - val_loss: 1.1832 - val_acc: 0.4333\n",
      "Epoch 74/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1864 - acc: 0.5690 - val_loss: 1.1777 - val_acc: 0.4333\n",
      "Epoch 75/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.1161 - acc: 0.5776 - val_loss: 1.1724 - val_acc: 0.4333\n",
      "Epoch 76/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.1097 - acc: 0.5862 - val_loss: 1.1672 - val_acc: 0.4333\n",
      "Epoch 77/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0370 - acc: 0.5776 - val_loss: 1.1626 - val_acc: 0.4333\n",
      "Epoch 78/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1131 - acc: 0.5776 - val_loss: 1.1575 - val_acc: 0.4333\n",
      "Epoch 79/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1238 - acc: 0.5776 - val_loss: 1.1526 - val_acc: 0.4333\n",
      "Epoch 80/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1006 - acc: 0.5776 - val_loss: 1.1476 - val_acc: 0.4333\n",
      "Epoch 81/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1223 - acc: 0.5948 - val_loss: 1.1421 - val_acc: 0.4333\n",
      "Epoch 82/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.0521 - acc: 0.5776 - val_loss: 1.1373 - val_acc: 0.4333\n",
      "Epoch 83/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0518 - acc: 0.5517 - val_loss: 1.1333 - val_acc: 0.4333\n",
      "Epoch 84/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1166 - acc: 0.5776 - val_loss: 1.1286 - val_acc: 0.4333\n",
      "Epoch 85/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0602 - acc: 0.5690 - val_loss: 1.1238 - val_acc: 0.4333\n",
      "Epoch 86/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0947 - acc: 0.5948 - val_loss: 1.1186 - val_acc: 0.4333\n",
      "Epoch 87/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0190 - acc: 0.5862 - val_loss: 1.1125 - val_acc: 0.4333\n",
      "Epoch 88/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0676 - acc: 0.5690 - val_loss: 1.1061 - val_acc: 0.4333\n",
      "Epoch 89/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0383 - acc: 0.5776 - val_loss: 1.1003 - val_acc: 0.4333\n",
      "Epoch 90/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0536 - acc: 0.5948 - val_loss: 1.0950 - val_acc: 0.4333\n",
      "Epoch 91/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0244 - acc: 0.6034 - val_loss: 1.0899 - val_acc: 0.4333\n",
      "Epoch 92/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0393 - acc: 0.5690 - val_loss: 1.0851 - val_acc: 0.4333\n",
      "Epoch 93/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0613 - acc: 0.5517 - val_loss: 1.0807 - val_acc: 0.4333\n",
      "Epoch 94/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1044 - acc: 0.5948 - val_loss: 1.0766 - val_acc: 0.4333\n",
      "Epoch 95/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0729 - acc: 0.5690 - val_loss: 1.0727 - val_acc: 0.4333\n",
      "Epoch 96/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0858 - acc: 0.6034 - val_loss: 1.0686 - val_acc: 0.4333\n",
      "Epoch 97/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0417 - acc: 0.5517 - val_loss: 1.0646 - val_acc: 0.4333\n",
      "Epoch 98/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0680 - acc: 0.5862 - val_loss: 1.0607 - val_acc: 0.4333\n",
      "Epoch 99/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.0887 - acc: 0.5948 - val_loss: 1.0573 - val_acc: 0.4333\n",
      "Epoch 100/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1169 - acc: 0.5776 - val_loss: 1.0547 - val_acc: 0.4333\n",
      "Epoch 101/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0211 - acc: 0.6034 - val_loss: 1.0514 - val_acc: 0.4333\n",
      "Epoch 102/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.0917 - acc: 0.5776 - val_loss: 1.0487 - val_acc: 0.4333\n",
      "Epoch 103/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0295 - acc: 0.6034 - val_loss: 1.0461 - val_acc: 0.4333\n",
      "Epoch 104/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9852 - acc: 0.5948 - val_loss: 1.0433 - val_acc: 0.4333\n",
      "Epoch 105/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9787 - acc: 0.5862 - val_loss: 1.0399 - val_acc: 0.4333\n",
      "Epoch 106/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.9888 - acc: 0.5862 - val_loss: 1.0366 - val_acc: 0.4333\n",
      "Epoch 107/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0436 - acc: 0.5690 - val_loss: 1.0338 - val_acc: 0.4333\n",
      "Epoch 108/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1167 - acc: 0.5603 - val_loss: 1.0314 - val_acc: 0.4667\n",
      "Epoch 109/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0417 - acc: 0.5776 - val_loss: 1.0289 - val_acc: 0.4667\n",
      "Epoch 110/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1041 - acc: 0.6121 - val_loss: 1.0265 - val_acc: 0.5000\n",
      "Epoch 111/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.0480 - acc: 0.5948 - val_loss: 1.0238 - val_acc: 0.5000\n",
      "Epoch 112/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0458 - acc: 0.5948 - val_loss: 1.0210 - val_acc: 0.5000\n",
      "Epoch 113/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.0161 - acc: 0.5948 - val_loss: 1.0179 - val_acc: 0.5000\n",
      "Epoch 114/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9841 - acc: 0.5948 - val_loss: 1.0146 - val_acc: 0.5000\n",
      "Epoch 115/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9931 - acc: 0.6034 - val_loss: 1.0111 - val_acc: 0.5333\n",
      "Epoch 116/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.9899 - acc: 0.5862 - val_loss: 1.0071 - val_acc: 0.5333\n",
      "Epoch 117/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0206 - acc: 0.6034 - val_loss: 1.0033 - val_acc: 0.5333\n",
      "Epoch 118/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0352 - acc: 0.6207 - val_loss: 0.9997 - val_acc: 0.6000\n",
      "Epoch 119/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0396 - acc: 0.5948 - val_loss: 0.9964 - val_acc: 0.6000\n",
      "Epoch 120/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9698 - acc: 0.6121 - val_loss: 0.9929 - val_acc: 0.6000\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 42us/step - loss: 0.9440 - acc: 0.5948 - val_loss: 0.9892 - val_acc: 0.6000\n",
      "Epoch 122/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9671 - acc: 0.6207 - val_loss: 0.9852 - val_acc: 0.6000\n",
      "Epoch 123/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9757 - acc: 0.6121 - val_loss: 0.9814 - val_acc: 0.6000\n",
      "Epoch 124/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 0.9775 - acc: 0.6034 - val_loss: 0.9777 - val_acc: 0.6333\n",
      "Epoch 125/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.0169 - acc: 0.6121 - val_loss: 0.9741 - val_acc: 0.6333\n",
      "Epoch 126/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 0.9728 - acc: 0.5690 - val_loss: 0.9702 - val_acc: 0.6333\n",
      "Epoch 127/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9684 - acc: 0.5948 - val_loss: 0.9660 - val_acc: 0.6333\n",
      "Epoch 128/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0243 - acc: 0.6121 - val_loss: 0.9630 - val_acc: 0.6333\n",
      "Epoch 129/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9682 - acc: 0.6121 - val_loss: 0.9599 - val_acc: 0.6333\n",
      "Epoch 130/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0159 - acc: 0.5948 - val_loss: 0.9565 - val_acc: 0.6333\n",
      "Epoch 131/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9446 - acc: 0.6034 - val_loss: 0.9531 - val_acc: 0.6333\n",
      "Epoch 132/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0095 - acc: 0.6034 - val_loss: 0.9500 - val_acc: 0.6333\n",
      "Epoch 133/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9166 - acc: 0.6293 - val_loss: 0.9467 - val_acc: 0.6333\n",
      "Epoch 134/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.0116 - acc: 0.6034 - val_loss: 0.9440 - val_acc: 0.6667\n",
      "Epoch 135/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9759 - acc: 0.5948 - val_loss: 0.9414 - val_acc: 0.6667\n",
      "Epoch 136/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0026 - acc: 0.6121 - val_loss: 0.9383 - val_acc: 0.6667\n",
      "Epoch 137/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9699 - acc: 0.6379 - val_loss: 0.9355 - val_acc: 0.6667\n",
      "Epoch 138/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.9963 - acc: 0.6293 - val_loss: 0.9331 - val_acc: 0.6667\n",
      "Epoch 139/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0063 - acc: 0.5862 - val_loss: 0.9307 - val_acc: 0.6667\n",
      "Epoch 140/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9487 - acc: 0.6207 - val_loss: 0.9280 - val_acc: 0.7000\n",
      "Epoch 141/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9750 - acc: 0.6121 - val_loss: 0.9253 - val_acc: 0.7000\n",
      "Epoch 142/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9466 - acc: 0.6034 - val_loss: 0.9231 - val_acc: 0.7000\n",
      "Epoch 143/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9592 - acc: 0.6379 - val_loss: 0.9205 - val_acc: 0.7000\n",
      "Epoch 144/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9397 - acc: 0.6466 - val_loss: 0.9179 - val_acc: 0.7000\n",
      "Epoch 145/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9428 - acc: 0.6121 - val_loss: 0.9150 - val_acc: 0.7000\n",
      "Epoch 146/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9707 - acc: 0.6121 - val_loss: 0.9128 - val_acc: 0.7000\n",
      "Epoch 147/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9133 - acc: 0.6207 - val_loss: 0.9108 - val_acc: 0.7000\n",
      "Epoch 148/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9523 - acc: 0.5862 - val_loss: 0.9099 - val_acc: 0.7000\n",
      "Epoch 149/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0178 - acc: 0.5948 - val_loss: 0.9107 - val_acc: 0.7000\n",
      "Epoch 150/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9493 - acc: 0.6379 - val_loss: 0.9113 - val_acc: 0.7000\n",
      "Epoch 151/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9559 - acc: 0.6207 - val_loss: 0.9130 - val_acc: 0.7000\n",
      "Epoch 152/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9468 - acc: 0.6207 - val_loss: 0.9137 - val_acc: 0.7000\n",
      "Epoch 153/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9103 - acc: 0.6207 - val_loss: 0.9138 - val_acc: 0.7000\n",
      "Epoch 154/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9438 - acc: 0.6207 - val_loss: 0.9147 - val_acc: 0.7000\n",
      "Epoch 155/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.9929 - acc: 0.6207 - val_loss: 0.9141 - val_acc: 0.7000\n",
      "Epoch 156/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9726 - acc: 0.6121 - val_loss: 0.9140 - val_acc: 0.7000\n",
      "Epoch 157/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9560 - acc: 0.6207 - val_loss: 0.9118 - val_acc: 0.7000\n",
      "Epoch 158/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9579 - acc: 0.5776 - val_loss: 0.9095 - val_acc: 0.7000\n",
      "Epoch 159/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0025 - acc: 0.6379 - val_loss: 0.9061 - val_acc: 0.7000\n",
      "Epoch 160/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0127 - acc: 0.6034 - val_loss: 0.9027 - val_acc: 0.7000\n",
      "Epoch 161/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9328 - acc: 0.6121 - val_loss: 0.9000 - val_acc: 0.7000\n",
      "Epoch 162/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 0.9286 - acc: 0.6207 - val_loss: 0.8969 - val_acc: 0.7000\n",
      "Epoch 163/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9073 - acc: 0.6293 - val_loss: 0.8936 - val_acc: 0.7000\n",
      "Epoch 164/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8409 - acc: 0.6379 - val_loss: 0.8897 - val_acc: 0.7000\n",
      "Epoch 165/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8969 - acc: 0.6207 - val_loss: 0.8861 - val_acc: 0.7000\n",
      "Epoch 166/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.9258 - acc: 0.6034 - val_loss: 0.8826 - val_acc: 0.7000\n",
      "Epoch 167/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8948 - acc: 0.6207 - val_loss: 0.8805 - val_acc: 0.7000\n",
      "Epoch 168/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9431 - acc: 0.6207 - val_loss: 0.8778 - val_acc: 0.7000\n",
      "Epoch 169/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9316 - acc: 0.6293 - val_loss: 0.8759 - val_acc: 0.7000\n",
      "Epoch 170/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9901 - acc: 0.6034 - val_loss: 0.8743 - val_acc: 0.7000\n",
      "Epoch 171/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9010 - acc: 0.6466 - val_loss: 0.8724 - val_acc: 0.7000\n",
      "Epoch 172/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8801 - acc: 0.6379 - val_loss: 0.8716 - val_acc: 0.7000\n",
      "Epoch 173/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9089 - acc: 0.5776 - val_loss: 0.8702 - val_acc: 0.7000\n",
      "Epoch 174/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9542 - acc: 0.5690 - val_loss: 0.8693 - val_acc: 0.7000\n",
      "Epoch 175/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9290 - acc: 0.6121 - val_loss: 0.8683 - val_acc: 0.7000\n",
      "Epoch 176/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9039 - acc: 0.5948 - val_loss: 0.8674 - val_acc: 0.7000\n",
      "Epoch 177/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9420 - acc: 0.6121 - val_loss: 0.8662 - val_acc: 0.7000\n",
      "Epoch 178/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9379 - acc: 0.6121 - val_loss: 0.8647 - val_acc: 0.7000\n",
      "Epoch 179/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9360 - acc: 0.6034 - val_loss: 0.8645 - val_acc: 0.7000\n",
      "Epoch 180/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.9474 - acc: 0.5776 - val_loss: 0.8652 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.9540 - acc: 0.5948 - val_loss: 0.8655 - val_acc: 0.7000\n",
      "Epoch 182/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.9554 - acc: 0.6552 - val_loss: 0.8674 - val_acc: 0.7000\n",
      "Epoch 183/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.9011 - acc: 0.6034 - val_loss: 0.8690 - val_acc: 0.7000\n",
      "Epoch 184/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9011 - acc: 0.6034 - val_loss: 0.8707 - val_acc: 0.7000\n",
      "Epoch 185/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.9166 - acc: 0.5948 - val_loss: 0.8715 - val_acc: 0.7000\n",
      "Epoch 186/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.9113 - acc: 0.6034 - val_loss: 0.8729 - val_acc: 0.7000\n",
      "Epoch 187/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8950 - acc: 0.5948 - val_loss: 0.8747 - val_acc: 0.7000\n",
      "Epoch 188/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 0.9377 - acc: 0.6034 - val_loss: 0.8766 - val_acc: 0.7000\n",
      "Epoch 189/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 0.9713 - acc: 0.6121 - val_loss: 0.8787 - val_acc: 0.7000\n",
      "Epoch 00189: early stopping\n"
     ]
    }
   ],
   "source": [
    "# モデル学習\n",
    "epochs = 1000\n",
    "batch_size = len(X_train)    #1:確率的勾配降下法　M:（<=N）ミニバッチ勾配降下法\n",
    "# 過学習（オーバーフィッティング）防止の為、前のエポックのときと比べ誤差が増えたら学習を打ち切る\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "hist = model.fit(X_train, Y_train, epochs=epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_data=(X_validation, Y_validation),\n",
    "                 callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1beb8550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 学習の進み具合を可視化\n",
    "\n",
    "val_acc = hist.history['val_acc']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "plt.rc('font', family='serif')\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(val_acc)), val_acc, label='val_acc', color='red')\n",
    "plt.plot(range(len(val_loss)), val_loss, label='val_loss', color='black')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 93us/step\n",
      "[0.84923212270478943, 0.67567567889754832]\n"
     ]
    }
   ],
   "source": [
    "# 予測精度の評価\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print(loss_and_metrics) # 1番目：誤差関数の値、2番目：予測精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 0 0 0 0 4 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# テスト用データを使用して予測する\n",
    "test_size = 10\n",
    "X_predict = np.argmax(model.predict(X_test[0:test_size]),axis=1)\n",
    "print(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 0 0 0 0 4 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# テスト用データの正解　0：資産　1：負債　2：純資産　3：収益　4：費用\n",
    "print(np.argmax(Y_test[0:test_size],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 予測が正しかったか判定する\n",
    "X_predict == np.argmax(Y_test[0:test_size],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1067pt\" viewBox=\"0.00 0.00 273.98 1067.00\" width=\"274pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1063)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-1063 269.9766,-1063 269.9766,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 112120841328 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>112120841328</title>\n",
       "<polygon fill=\"none\" points=\"49.3691,-1022.5 49.3691,-1058.5 216.6074,-1058.5 216.6074,-1022.5 49.3691,-1022.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-1036.3\">dense_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 112120840992 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>112120840992</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-949.5 80.8623,-985.5 185.1143,-985.5 185.1143,-949.5 80.8623,-949.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-963.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 112120841328&#45;&gt;112120840992 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>112120841328-&gt;112120840992</title>\n",
       "<path d=\"M132.9883,-1022.4551C132.9883,-1014.3828 132.9883,-1004.6764 132.9883,-995.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-995.5903 132.9883,-985.5904 129.4884,-995.5904 136.4884,-995.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112120891936 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>112120891936</title>\n",
       "<polygon fill=\"none\" points=\"0,-876.5 0,-912.5 265.9766,-912.5 265.9766,-876.5 0,-876.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-890.3\">batch_normalization_1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 112120840992&#45;&gt;112120891936 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>112120840992-&gt;112120891936</title>\n",
       "<path d=\"M132.9883,-949.4551C132.9883,-941.3828 132.9883,-931.6764 132.9883,-922.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-922.5903 132.9883,-912.5904 129.4884,-922.5904 136.4884,-922.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112120893168 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>112120893168</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-803.5 57.5278,-839.5 208.4487,-839.5 208.4487,-803.5 57.5278,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-817.3\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 112120891936&#45;&gt;112120893168 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>112120891936-&gt;112120893168</title>\n",
       "<path d=\"M132.9883,-876.4551C132.9883,-868.3828 132.9883,-858.6764 132.9883,-849.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-849.5903 132.9883,-839.5904 129.4884,-849.5904 136.4884,-849.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112120842280 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>112120842280</title>\n",
       "<polygon fill=\"none\" points=\"69.1865,-730.5 69.1865,-766.5 196.79,-766.5 196.79,-730.5 69.1865,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-744.3\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 112120893168&#45;&gt;112120842280 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>112120893168-&gt;112120842280</title>\n",
       "<path d=\"M132.9883,-803.4551C132.9883,-795.3828 132.9883,-785.6764 132.9883,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-776.5903 132.9883,-766.5904 129.4884,-776.5904 136.4884,-776.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112120843848 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>112120843848</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-657.5 80.8623,-693.5 185.1143,-693.5 185.1143,-657.5 80.8623,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-671.3\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 112120842280&#45;&gt;112120843848 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>112120842280-&gt;112120843848</title>\n",
       "<path d=\"M132.9883,-730.4551C132.9883,-722.3828 132.9883,-712.6764 132.9883,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-703.5903 132.9883,-693.5904 129.4884,-703.5904 136.4884,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112121238248 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>112121238248</title>\n",
       "<polygon fill=\"none\" points=\"0,-584.5 0,-620.5 265.9766,-620.5 265.9766,-584.5 0,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-598.3\">batch_normalization_2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 112120843848&#45;&gt;112121238248 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>112120843848-&gt;112121238248</title>\n",
       "<path d=\"M132.9883,-657.4551C132.9883,-649.3828 132.9883,-639.6764 132.9883,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-630.5903 132.9883,-620.5904 129.4884,-630.5904 136.4884,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112121531976 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>112121531976</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-511.5 57.5278,-547.5 208.4487,-547.5 208.4487,-511.5 57.5278,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-525.3\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 112121238248&#45;&gt;112121531976 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>112121238248-&gt;112121531976</title>\n",
       "<path d=\"M132.9883,-584.4551C132.9883,-576.3828 132.9883,-566.6764 132.9883,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-557.5903 132.9883,-547.5904 129.4884,-557.5904 136.4884,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112121654688 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>112121654688</title>\n",
       "<polygon fill=\"none\" points=\"69.1865,-438.5 69.1865,-474.5 196.79,-474.5 196.79,-438.5 69.1865,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-452.3\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 112121531976&#45;&gt;112121654688 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>112121531976-&gt;112121654688</title>\n",
       "<path d=\"M132.9883,-511.4551C132.9883,-503.3828 132.9883,-493.6764 132.9883,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-484.5903 132.9883,-474.5904 129.4884,-484.5904 136.4884,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112121655136 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>112121655136</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-365.5 80.8623,-401.5 185.1143,-401.5 185.1143,-365.5 80.8623,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-379.3\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 112121654688&#45;&gt;112121655136 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>112121654688-&gt;112121655136</title>\n",
       "<path d=\"M132.9883,-438.4551C132.9883,-430.3828 132.9883,-420.6764 132.9883,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-411.5903 132.9883,-401.5904 129.4884,-411.5904 136.4884,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112121836880 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>112121836880</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 265.9766,-328.5 265.9766,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-306.3\">batch_normalization_3: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 112121655136&#45;&gt;112121836880 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>112121655136-&gt;112121836880</title>\n",
       "<path d=\"M132.9883,-365.4551C132.9883,-357.3828 132.9883,-347.6764 132.9883,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-338.5903 132.9883,-328.5904 129.4884,-338.5904 136.4884,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112122437304 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>112122437304</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-219.5 57.5278,-255.5 208.4487,-255.5 208.4487,-219.5 57.5278,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-233.3\">activation_3: Activation</text>\n",
       "</g>\n",
       "<!-- 112121836880&#45;&gt;112122437304 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>112121836880-&gt;112122437304</title>\n",
       "<path d=\"M132.9883,-292.4551C132.9883,-284.3828 132.9883,-274.6764 132.9883,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-265.5903 132.9883,-255.5904 129.4884,-265.5904 136.4884,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112122372048 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>112122372048</title>\n",
       "<polygon fill=\"none\" points=\"69.1865,-146.5 69.1865,-182.5 196.79,-182.5 196.79,-146.5 69.1865,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-160.3\">dropout_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 112122437304&#45;&gt;112122372048 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>112122437304-&gt;112122372048</title>\n",
       "<path d=\"M132.9883,-219.4551C132.9883,-211.3828 132.9883,-201.6764 132.9883,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-192.5903 132.9883,-182.5904 129.4884,-192.5904 136.4884,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112120840768 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>112120840768</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-73.5 80.8623,-109.5 185.1143,-109.5 185.1143,-73.5 80.8623,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-87.3\">dense_4: Dense</text>\n",
       "</g>\n",
       "<!-- 112122372048&#45;&gt;112120840768 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>112122372048-&gt;112120840768</title>\n",
       "<path d=\"M132.9883,-146.4551C132.9883,-138.3828 132.9883,-128.6764 132.9883,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-119.5903 132.9883,-109.5904 129.4884,-119.5904 136.4884,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112120840936 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>112120840936</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-.5 57.5278,-36.5 208.4487,-36.5 208.4487,-.5 57.5278,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-14.3\">activation_4: Activation</text>\n",
       "</g>\n",
       "<!-- 112120840768&#45;&gt;112120840936 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>112120840768-&gt;112120840936</title>\n",
       "<path d=\"M132.9883,-73.4551C132.9883,-65.3828 132.9883,-55.6764 132.9883,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-46.5903 132.9883,-36.5904 129.4884,-46.5904 136.4884,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
