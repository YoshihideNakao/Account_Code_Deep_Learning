{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_file = open(\"mnist_dataset/勘定科目コード.csv\",'r')\n",
    "account_data = account_file.readlines()\n",
    "account_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込んだデータをPythonリスト形式に変換して、先頭のラベルと2桁目以降の画像データを分離する\n",
    "n = len(account_data)\n",
    "account_list = []\n",
    "account_target = []\n",
    "for i in range(n):\n",
    "    account_list.append(list(map(int, account_data[i].split(',')[0])))\n",
    "    account_target.append(int(account_data[i].split(',')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用にデータを抽出する\n",
    "#N = 30000 # MNISTの一部のデータで実験\n",
    "# indices = np.random.permutation(range(n))[:N]\n",
    "#X = []\n",
    "#y = []\n",
    "#for i in indices:\n",
    "#    X.append(mnist_list[i])\n",
    "#    y.append(mnist_target[i])\n",
    "\n",
    "X = account_list\n",
    "y = account_target\n",
    "Y = np.eye(5)[y] # 1-of-k表現に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの正規化\n",
    "X = np.array(X)\n",
    "X = X / X.max()\n",
    "X = X - X.mean(axis=1).reshape(len(X), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "N_train = 0.8\n",
    "N_validation =0.2\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=N_train)\n",
    "\n",
    "# 訓練データをさらに訓練データと検証データに分類\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size = N_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 445\n",
      "Trainable params: 385\n",
      "Non-trainable params: 60\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# モデル設定\n",
    "n_in = len(X[0])    # 入力層のノード数\n",
    "n_hiddens = [10,10,10]   # 隠れ層のノード数\n",
    "n_out = len(Y[0])   # 出力層のノード数\n",
    "activation = 'relu' # 活性化関数　出力の値を0~1の確率変数に置き換える 'sigmoid''tanh''relu'　\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    return np.sqrt(2.0 / shape[0]) * np.random.normal(size=shape)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 入力層、隠れ層を生成する\n",
    "for i, input_dim in enumerate(([n_in] + n_hiddens)[:-1]):\n",
    "    model.add(Dense(n_hiddens[i], input_dim=input_dim,\n",
    "                    kernel_initializer=weight_variable))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.5)) # オーバーフィッティング対策　学習時に間引くノードの確率\n",
    "\n",
    "# 出力層を生成する  \n",
    "model.add(Dense(n_out, kernel_initializer=weight_variable))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 学習の準備　optimizer:勾配法　loss:誤差関数　metrics:評価関数\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.01, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 116 samples, validate on 30 samples\n",
      "Epoch 1/1000\n",
      "116/116 [==============================] - 2s 13ms/step - loss: 2.5412 - acc: 0.1466 - val_loss: 1.6308 - val_acc: 0.2667\n",
      "Epoch 2/1000\n",
      "116/116 [==============================] - 0s 48us/step - loss: 2.5004 - acc: 0.1810 - val_loss: 1.6004 - val_acc: 0.5000\n",
      "Epoch 3/1000\n",
      "116/116 [==============================] - 0s 49us/step - loss: 2.1768 - acc: 0.2759 - val_loss: 1.5710 - val_acc: 0.5333\n",
      "Epoch 4/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 2.3423 - acc: 0.2586 - val_loss: 1.5491 - val_acc: 0.5333\n",
      "Epoch 5/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.9927 - acc: 0.3103 - val_loss: 1.5308 - val_acc: 0.5000\n",
      "Epoch 6/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 1.9197 - acc: 0.3276 - val_loss: 1.5156 - val_acc: 0.5000\n",
      "Epoch 7/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 2.0968 - acc: 0.2845 - val_loss: 1.5023 - val_acc: 0.5000\n",
      "Epoch 8/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.7818 - acc: 0.3448 - val_loss: 1.4900 - val_acc: 0.4667\n",
      "Epoch 9/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.7973 - acc: 0.3621 - val_loss: 1.4783 - val_acc: 0.4667\n",
      "Epoch 10/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 2.0589 - acc: 0.3190 - val_loss: 1.4673 - val_acc: 0.4667\n",
      "Epoch 11/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.7230 - acc: 0.4052 - val_loss: 1.4561 - val_acc: 0.4667\n",
      "Epoch 12/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.6898 - acc: 0.3879 - val_loss: 1.4476 - val_acc: 0.4667\n",
      "Epoch 13/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.5920 - acc: 0.3966 - val_loss: 1.4392 - val_acc: 0.4667\n",
      "Epoch 14/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.6962 - acc: 0.3362 - val_loss: 1.4312 - val_acc: 0.4667\n",
      "Epoch 15/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.5831 - acc: 0.4224 - val_loss: 1.4232 - val_acc: 0.4667\n",
      "Epoch 16/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.6739 - acc: 0.3879 - val_loss: 1.4170 - val_acc: 0.4667\n",
      "Epoch 17/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.4852 - acc: 0.4483 - val_loss: 1.4107 - val_acc: 0.4667\n",
      "Epoch 18/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.5041 - acc: 0.4310 - val_loss: 1.4058 - val_acc: 0.4667\n",
      "Epoch 19/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.4327 - acc: 0.4741 - val_loss: 1.4014 - val_acc: 0.4667\n",
      "Epoch 20/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.5161 - acc: 0.4397 - val_loss: 1.3963 - val_acc: 0.4667\n",
      "Epoch 21/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.4290 - acc: 0.4741 - val_loss: 1.3923 - val_acc: 0.4667\n",
      "Epoch 22/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.3704 - acc: 0.5345 - val_loss: 1.3871 - val_acc: 0.4667\n",
      "Epoch 23/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.3724 - acc: 0.5345 - val_loss: 1.3820 - val_acc: 0.4667\n",
      "Epoch 24/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.2745 - acc: 0.5172 - val_loss: 1.3776 - val_acc: 0.4667\n",
      "Epoch 25/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.3408 - acc: 0.5259 - val_loss: 1.3724 - val_acc: 0.4667\n",
      "Epoch 26/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.3605 - acc: 0.5259 - val_loss: 1.3683 - val_acc: 0.4667\n",
      "Epoch 27/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.3072 - acc: 0.5690 - val_loss: 1.3652 - val_acc: 0.4667\n",
      "Epoch 28/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.2707 - acc: 0.5603 - val_loss: 1.3610 - val_acc: 0.4667\n",
      "Epoch 29/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.2199 - acc: 0.5690 - val_loss: 1.3564 - val_acc: 0.4667\n",
      "Epoch 30/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.3163 - acc: 0.5690 - val_loss: 1.3527 - val_acc: 0.4667\n",
      "Epoch 31/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.2800 - acc: 0.5603 - val_loss: 1.3487 - val_acc: 0.4667\n",
      "Epoch 32/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.3066 - acc: 0.5172 - val_loss: 1.3463 - val_acc: 0.4667\n",
      "Epoch 33/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.1973 - acc: 0.5776 - val_loss: 1.3444 - val_acc: 0.4667\n",
      "Epoch 34/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.2413 - acc: 0.5862 - val_loss: 1.3426 - val_acc: 0.4667\n",
      "Epoch 35/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.2742 - acc: 0.5259 - val_loss: 1.3398 - val_acc: 0.4667\n",
      "Epoch 36/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.2496 - acc: 0.5776 - val_loss: 1.3367 - val_acc: 0.4667\n",
      "Epoch 37/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.1643 - acc: 0.5862 - val_loss: 1.3340 - val_acc: 0.4667\n",
      "Epoch 38/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.2028 - acc: 0.5862 - val_loss: 1.3324 - val_acc: 0.4667\n",
      "Epoch 39/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.2644 - acc: 0.5517 - val_loss: 1.3309 - val_acc: 0.4667\n",
      "Epoch 40/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.2733 - acc: 0.5690 - val_loss: 1.3285 - val_acc: 0.4667\n",
      "Epoch 41/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1951 - acc: 0.5690 - val_loss: 1.3261 - val_acc: 0.4667\n",
      "Epoch 42/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.1817 - acc: 0.5862 - val_loss: 1.3231 - val_acc: 0.4667\n",
      "Epoch 43/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1514 - acc: 0.5948 - val_loss: 1.3200 - val_acc: 0.4667\n",
      "Epoch 44/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1539 - acc: 0.5776 - val_loss: 1.3167 - val_acc: 0.4667\n",
      "Epoch 45/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.1983 - acc: 0.5862 - val_loss: 1.3132 - val_acc: 0.4667\n",
      "Epoch 46/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.2177 - acc: 0.5776 - val_loss: 1.3090 - val_acc: 0.4667\n",
      "Epoch 47/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1748 - acc: 0.5862 - val_loss: 1.3050 - val_acc: 0.4667\n",
      "Epoch 48/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.2010 - acc: 0.5690 - val_loss: 1.3010 - val_acc: 0.4667\n",
      "Epoch 49/1000\n",
      "116/116 [==============================] - 0s 47us/step - loss: 1.1434 - acc: 0.6034 - val_loss: 1.2962 - val_acc: 0.4667\n",
      "Epoch 50/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1810 - acc: 0.5776 - val_loss: 1.2919 - val_acc: 0.4667\n",
      "Epoch 51/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1621 - acc: 0.5948 - val_loss: 1.2871 - val_acc: 0.4667\n",
      "Epoch 52/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1749 - acc: 0.5603 - val_loss: 1.2824 - val_acc: 0.4667\n",
      "Epoch 53/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 1.0851 - acc: 0.6034 - val_loss: 1.2774 - val_acc: 0.4667\n",
      "Epoch 54/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 1.1303 - acc: 0.5948 - val_loss: 1.2727 - val_acc: 0.4667\n",
      "Epoch 55/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1161 - acc: 0.5776 - val_loss: 1.2672 - val_acc: 0.4667\n",
      "Epoch 56/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1719 - acc: 0.5603 - val_loss: 1.2619 - val_acc: 0.4667\n",
      "Epoch 57/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1085 - acc: 0.5776 - val_loss: 1.2575 - val_acc: 0.4667\n",
      "Epoch 58/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.1545 - acc: 0.5948 - val_loss: 1.2524 - val_acc: 0.4667\n",
      "Epoch 59/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 1.1802 - acc: 0.5776 - val_loss: 1.2469 - val_acc: 0.4667\n",
      "Epoch 60/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1274 - acc: 0.5776 - val_loss: 1.2421 - val_acc: 0.4667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 1.1118 - acc: 0.6034 - val_loss: 1.2368 - val_acc: 0.4667\n",
      "Epoch 62/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1428 - acc: 0.5948 - val_loss: 1.2311 - val_acc: 0.4667\n",
      "Epoch 63/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 1.1012 - acc: 0.5862 - val_loss: 1.2250 - val_acc: 0.4667\n",
      "Epoch 64/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1590 - acc: 0.5862 - val_loss: 1.2199 - val_acc: 0.4667\n",
      "Epoch 65/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.1212 - acc: 0.6034 - val_loss: 1.2149 - val_acc: 0.4667\n",
      "Epoch 66/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.1359 - acc: 0.5776 - val_loss: 1.2110 - val_acc: 0.4667\n",
      "Epoch 67/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0839 - acc: 0.6034 - val_loss: 1.2074 - val_acc: 0.4667\n",
      "Epoch 68/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 1.0765 - acc: 0.5690 - val_loss: 1.2021 - val_acc: 0.4667\n",
      "Epoch 69/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.1446 - acc: 0.5690 - val_loss: 1.1979 - val_acc: 0.4667\n",
      "Epoch 70/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 1.0396 - acc: 0.5948 - val_loss: 1.1934 - val_acc: 0.4667\n",
      "Epoch 71/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.0604 - acc: 0.6121 - val_loss: 1.1881 - val_acc: 0.4667\n",
      "Epoch 72/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 1.1396 - acc: 0.5948 - val_loss: 1.1835 - val_acc: 0.4667\n",
      "Epoch 73/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0663 - acc: 0.5948 - val_loss: 1.1782 - val_acc: 0.4667\n",
      "Epoch 74/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.1000 - acc: 0.5776 - val_loss: 1.1726 - val_acc: 0.4667\n",
      "Epoch 75/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0667 - acc: 0.5862 - val_loss: 1.1672 - val_acc: 0.4667\n",
      "Epoch 76/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0888 - acc: 0.5776 - val_loss: 1.1615 - val_acc: 0.4667\n",
      "Epoch 77/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.1303 - acc: 0.5690 - val_loss: 1.1569 - val_acc: 0.4667\n",
      "Epoch 78/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 1.1259 - acc: 0.5948 - val_loss: 1.1515 - val_acc: 0.4667\n",
      "Epoch 79/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.0778 - acc: 0.5948 - val_loss: 1.1458 - val_acc: 0.4667\n",
      "Epoch 80/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1414 - acc: 0.5948 - val_loss: 1.1405 - val_acc: 0.4667\n",
      "Epoch 81/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.9958 - acc: 0.5948 - val_loss: 1.1347 - val_acc: 0.4667\n",
      "Epoch 82/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0845 - acc: 0.5862 - val_loss: 1.1295 - val_acc: 0.4667\n",
      "Epoch 83/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1285 - acc: 0.5776 - val_loss: 1.1247 - val_acc: 0.4667\n",
      "Epoch 84/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.0519 - acc: 0.5948 - val_loss: 1.1199 - val_acc: 0.4667\n",
      "Epoch 85/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.1116 - acc: 0.5690 - val_loss: 1.1154 - val_acc: 0.4667\n",
      "Epoch 86/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0787 - acc: 0.5690 - val_loss: 1.1111 - val_acc: 0.4667\n",
      "Epoch 87/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0955 - acc: 0.6121 - val_loss: 1.1069 - val_acc: 0.4667\n",
      "Epoch 88/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0725 - acc: 0.5862 - val_loss: 1.1022 - val_acc: 0.4667\n",
      "Epoch 89/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0374 - acc: 0.6034 - val_loss: 1.0982 - val_acc: 0.4667\n",
      "Epoch 90/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.0374 - acc: 0.5862 - val_loss: 1.0939 - val_acc: 0.4667\n",
      "Epoch 91/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0608 - acc: 0.6034 - val_loss: 1.0893 - val_acc: 0.4667\n",
      "Epoch 92/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0489 - acc: 0.5948 - val_loss: 1.0844 - val_acc: 0.4667\n",
      "Epoch 93/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0786 - acc: 0.5862 - val_loss: 1.0798 - val_acc: 0.4667\n",
      "Epoch 94/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0112 - acc: 0.5948 - val_loss: 1.0762 - val_acc: 0.4667\n",
      "Epoch 95/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0440 - acc: 0.5948 - val_loss: 1.0732 - val_acc: 0.4667\n",
      "Epoch 96/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.0459 - acc: 0.5690 - val_loss: 1.0694 - val_acc: 0.5000\n",
      "Epoch 97/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0371 - acc: 0.5948 - val_loss: 1.0655 - val_acc: 0.5000\n",
      "Epoch 98/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9631 - acc: 0.6034 - val_loss: 1.0615 - val_acc: 0.5333\n",
      "Epoch 99/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9950 - acc: 0.6034 - val_loss: 1.0582 - val_acc: 0.5333\n",
      "Epoch 100/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.1046 - acc: 0.5776 - val_loss: 1.0554 - val_acc: 0.5333\n",
      "Epoch 101/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9693 - acc: 0.6121 - val_loss: 1.0520 - val_acc: 0.5333\n",
      "Epoch 102/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9579 - acc: 0.6379 - val_loss: 1.0480 - val_acc: 0.5333\n",
      "Epoch 103/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9922 - acc: 0.6207 - val_loss: 1.0436 - val_acc: 0.5333\n",
      "Epoch 104/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0496 - acc: 0.6121 - val_loss: 1.0392 - val_acc: 0.5667\n",
      "Epoch 105/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.0412 - acc: 0.5862 - val_loss: 1.0351 - val_acc: 0.5667\n",
      "Epoch 106/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0519 - acc: 0.5948 - val_loss: 1.0327 - val_acc: 0.5667\n",
      "Epoch 107/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9487 - acc: 0.6207 - val_loss: 1.0308 - val_acc: 0.5667\n",
      "Epoch 108/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9957 - acc: 0.6121 - val_loss: 1.0280 - val_acc: 0.5667\n",
      "Epoch 109/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0034 - acc: 0.6034 - val_loss: 1.0250 - val_acc: 0.6000\n",
      "Epoch 110/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9991 - acc: 0.6034 - val_loss: 1.0220 - val_acc: 0.6000\n",
      "Epoch 111/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9589 - acc: 0.6121 - val_loss: 1.0186 - val_acc: 0.6000\n",
      "Epoch 112/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0061 - acc: 0.5948 - val_loss: 1.0159 - val_acc: 0.6000\n",
      "Epoch 113/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0043 - acc: 0.6207 - val_loss: 1.0131 - val_acc: 0.6000\n",
      "Epoch 114/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9862 - acc: 0.6121 - val_loss: 1.0112 - val_acc: 0.6333\n",
      "Epoch 115/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9437 - acc: 0.6207 - val_loss: 1.0089 - val_acc: 0.6667\n",
      "Epoch 116/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9141 - acc: 0.6466 - val_loss: 1.0067 - val_acc: 0.6667\n",
      "Epoch 117/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9355 - acc: 0.6121 - val_loss: 1.0041 - val_acc: 0.6667\n",
      "Epoch 118/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9827 - acc: 0.6293 - val_loss: 1.0021 - val_acc: 0.6667\n",
      "Epoch 119/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9816 - acc: 0.6207 - val_loss: 0.9996 - val_acc: 0.6667\n",
      "Epoch 120/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9616 - acc: 0.6121 - val_loss: 0.9967 - val_acc: 0.6667\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 44us/step - loss: 0.9542 - acc: 0.6207 - val_loss: 0.9927 - val_acc: 0.6667\n",
      "Epoch 122/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9642 - acc: 0.6034 - val_loss: 0.9885 - val_acc: 0.6667\n",
      "Epoch 123/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9310 - acc: 0.6293 - val_loss: 0.9838 - val_acc: 0.6667\n",
      "Epoch 124/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9386 - acc: 0.6121 - val_loss: 0.9787 - val_acc: 0.6667\n",
      "Epoch 125/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0118 - acc: 0.5948 - val_loss: 0.9743 - val_acc: 0.6667\n",
      "Epoch 126/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.0544 - acc: 0.6121 - val_loss: 0.9695 - val_acc: 0.6667\n",
      "Epoch 127/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9781 - acc: 0.6379 - val_loss: 0.9650 - val_acc: 0.6667\n",
      "Epoch 128/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8828 - acc: 0.6724 - val_loss: 0.9601 - val_acc: 0.6667\n",
      "Epoch 129/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.9586 - acc: 0.6379 - val_loss: 0.9552 - val_acc: 0.6667\n",
      "Epoch 130/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0004 - acc: 0.6638 - val_loss: 0.9508 - val_acc: 0.6667\n",
      "Epoch 131/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9962 - acc: 0.5948 - val_loss: 0.9473 - val_acc: 0.6667\n",
      "Epoch 132/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.9618 - acc: 0.6293 - val_loss: 0.9440 - val_acc: 0.6667\n",
      "Epoch 133/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0047 - acc: 0.6207 - val_loss: 0.9405 - val_acc: 0.6667\n",
      "Epoch 134/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9168 - acc: 0.6466 - val_loss: 0.9374 - val_acc: 0.6667\n",
      "Epoch 135/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9524 - acc: 0.6293 - val_loss: 0.9339 - val_acc: 0.6667\n",
      "Epoch 136/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9388 - acc: 0.6121 - val_loss: 0.9303 - val_acc: 0.6667\n",
      "Epoch 137/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0262 - acc: 0.6121 - val_loss: 0.9279 - val_acc: 0.6667\n",
      "Epoch 138/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8848 - acc: 0.6379 - val_loss: 0.9262 - val_acc: 0.6667\n",
      "Epoch 139/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9478 - acc: 0.6379 - val_loss: 0.9244 - val_acc: 0.6667\n",
      "Epoch 140/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8808 - acc: 0.6724 - val_loss: 0.9210 - val_acc: 0.6667\n",
      "Epoch 141/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9427 - acc: 0.6379 - val_loss: 0.9175 - val_acc: 0.6667\n",
      "Epoch 142/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9129 - acc: 0.6466 - val_loss: 0.9141 - val_acc: 0.6667\n",
      "Epoch 143/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8917 - acc: 0.6293 - val_loss: 0.9107 - val_acc: 0.6667\n",
      "Epoch 144/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9187 - acc: 0.6466 - val_loss: 0.9077 - val_acc: 0.6667\n",
      "Epoch 145/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9280 - acc: 0.6293 - val_loss: 0.9045 - val_acc: 0.6667\n",
      "Epoch 146/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.9027 - acc: 0.6379 - val_loss: 0.9018 - val_acc: 0.6667\n",
      "Epoch 147/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.8821 - acc: 0.6207 - val_loss: 0.8991 - val_acc: 0.6667\n",
      "Epoch 148/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8805 - acc: 0.6552 - val_loss: 0.8961 - val_acc: 0.6667\n",
      "Epoch 149/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8784 - acc: 0.6724 - val_loss: 0.8931 - val_acc: 0.6667\n",
      "Epoch 150/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8666 - acc: 0.6293 - val_loss: 0.8902 - val_acc: 0.6667\n",
      "Epoch 151/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8835 - acc: 0.6552 - val_loss: 0.8864 - val_acc: 0.6667\n",
      "Epoch 152/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9221 - acc: 0.6552 - val_loss: 0.8827 - val_acc: 0.6667\n",
      "Epoch 153/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9223 - acc: 0.6552 - val_loss: 0.8793 - val_acc: 0.6667\n",
      "Epoch 154/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9182 - acc: 0.6724 - val_loss: 0.8771 - val_acc: 0.6667\n",
      "Epoch 155/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9196 - acc: 0.6207 - val_loss: 0.8758 - val_acc: 0.6667\n",
      "Epoch 156/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9310 - acc: 0.6638 - val_loss: 0.8741 - val_acc: 0.6667\n",
      "Epoch 157/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9158 - acc: 0.6379 - val_loss: 0.8732 - val_acc: 0.6667\n",
      "Epoch 158/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8405 - acc: 0.6379 - val_loss: 0.8718 - val_acc: 0.6667\n",
      "Epoch 159/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.9286 - acc: 0.6638 - val_loss: 0.8711 - val_acc: 0.6667\n",
      "Epoch 160/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9098 - acc: 0.6552 - val_loss: 0.8702 - val_acc: 0.6667\n",
      "Epoch 161/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8385 - acc: 0.6724 - val_loss: 0.8692 - val_acc: 0.6667\n",
      "Epoch 162/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9313 - acc: 0.6552 - val_loss: 0.8677 - val_acc: 0.6667\n",
      "Epoch 163/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9024 - acc: 0.6466 - val_loss: 0.8665 - val_acc: 0.6667\n",
      "Epoch 164/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8962 - acc: 0.6552 - val_loss: 0.8654 - val_acc: 0.6667\n",
      "Epoch 165/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8680 - acc: 0.6638 - val_loss: 0.8637 - val_acc: 0.6667\n",
      "Epoch 166/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8979 - acc: 0.6121 - val_loss: 0.8617 - val_acc: 0.6667\n",
      "Epoch 167/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8741 - acc: 0.6552 - val_loss: 0.8587 - val_acc: 0.6667\n",
      "Epoch 168/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8771 - acc: 0.6638 - val_loss: 0.8555 - val_acc: 0.6667\n",
      "Epoch 169/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.8247 - acc: 0.6724 - val_loss: 0.8518 - val_acc: 0.6667\n",
      "Epoch 170/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.8781 - acc: 0.6466 - val_loss: 0.8483 - val_acc: 0.6667\n",
      "Epoch 171/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8646 - acc: 0.6552 - val_loss: 0.8445 - val_acc: 0.6667\n",
      "Epoch 172/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8081 - acc: 0.6983 - val_loss: 0.8411 - val_acc: 0.6667\n",
      "Epoch 173/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.8009 - acc: 0.6983 - val_loss: 0.8379 - val_acc: 0.6667\n",
      "Epoch 174/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8904 - acc: 0.6293 - val_loss: 0.8347 - val_acc: 0.6667\n",
      "Epoch 175/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8816 - acc: 0.6379 - val_loss: 0.8317 - val_acc: 0.6667\n",
      "Epoch 176/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.7949 - acc: 0.6638 - val_loss: 0.8287 - val_acc: 0.6667\n",
      "Epoch 177/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8904 - acc: 0.6552 - val_loss: 0.8263 - val_acc: 0.6667\n",
      "Epoch 178/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 0.8583 - acc: 0.6552 - val_loss: 0.8232 - val_acc: 0.6667\n",
      "Epoch 179/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8835 - acc: 0.6724 - val_loss: 0.8200 - val_acc: 0.6667\n",
      "Epoch 180/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8278 - acc: 0.6810 - val_loss: 0.8167 - val_acc: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.8773 - acc: 0.6724 - val_loss: 0.8128 - val_acc: 0.6667\n",
      "Epoch 182/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.8429 - acc: 0.6552 - val_loss: 0.8097 - val_acc: 0.6667\n",
      "Epoch 183/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8491 - acc: 0.6983 - val_loss: 0.8077 - val_acc: 0.6667\n",
      "Epoch 184/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8542 - acc: 0.6724 - val_loss: 0.8053 - val_acc: 0.6667\n",
      "Epoch 185/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8278 - acc: 0.6810 - val_loss: 0.8029 - val_acc: 0.7000\n",
      "Epoch 186/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.8231 - acc: 0.6466 - val_loss: 0.8007 - val_acc: 0.7000\n",
      "Epoch 187/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.8669 - acc: 0.6466 - val_loss: 0.7983 - val_acc: 0.7000\n",
      "Epoch 188/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.7650 - acc: 0.7069 - val_loss: 0.7957 - val_acc: 0.7000\n",
      "Epoch 189/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8322 - acc: 0.7155 - val_loss: 0.7935 - val_acc: 0.7000\n",
      "Epoch 190/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 0.8661 - acc: 0.6638 - val_loss: 0.7919 - val_acc: 0.7000\n",
      "Epoch 191/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.7841 - acc: 0.7241 - val_loss: 0.7910 - val_acc: 0.7000\n",
      "Epoch 192/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8674 - acc: 0.6552 - val_loss: 0.7902 - val_acc: 0.7333\n",
      "Epoch 193/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8693 - acc: 0.6897 - val_loss: 0.7890 - val_acc: 0.7333\n",
      "Epoch 194/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8334 - acc: 0.6810 - val_loss: 0.7867 - val_acc: 0.7333\n",
      "Epoch 195/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8580 - acc: 0.6466 - val_loss: 0.7831 - val_acc: 0.7333\n",
      "Epoch 196/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.7813 - acc: 0.6379 - val_loss: 0.7801 - val_acc: 0.7333\n",
      "Epoch 197/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.7970 - acc: 0.7241 - val_loss: 0.7769 - val_acc: 0.7333\n",
      "Epoch 198/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8392 - acc: 0.6638 - val_loss: 0.7745 - val_acc: 0.7333\n",
      "Epoch 199/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8630 - acc: 0.6897 - val_loss: 0.7724 - val_acc: 0.7333\n",
      "Epoch 200/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.8352 - acc: 0.6983 - val_loss: 0.7699 - val_acc: 0.7333\n",
      "Epoch 201/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 0.7504 - acc: 0.7328 - val_loss: 0.7668 - val_acc: 0.7333\n",
      "Epoch 202/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8308 - acc: 0.7414 - val_loss: 0.7643 - val_acc: 0.7333\n",
      "Epoch 203/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.7818 - acc: 0.7328 - val_loss: 0.7615 - val_acc: 0.7333\n",
      "Epoch 204/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.7861 - acc: 0.7241 - val_loss: 0.7587 - val_acc: 0.7333\n",
      "Epoch 205/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.8338 - acc: 0.6724 - val_loss: 0.7569 - val_acc: 0.7333\n",
      "Epoch 206/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8943 - acc: 0.6724 - val_loss: 0.7559 - val_acc: 0.7333\n",
      "Epoch 207/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.8040 - acc: 0.7155 - val_loss: 0.7542 - val_acc: 0.7333\n",
      "Epoch 208/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.8537 - acc: 0.6638 - val_loss: 0.7531 - val_acc: 0.7333\n",
      "Epoch 209/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.7548 - acc: 0.6897 - val_loss: 0.7524 - val_acc: 0.7333\n",
      "Epoch 210/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8380 - acc: 0.6983 - val_loss: 0.7516 - val_acc: 0.7333\n",
      "Epoch 211/1000\n",
      "116/116 [==============================] - 0s 32us/step - loss: 0.7564 - acc: 0.6897 - val_loss: 0.7507 - val_acc: 0.7000\n",
      "Epoch 212/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.8521 - acc: 0.6897 - val_loss: 0.7494 - val_acc: 0.7000\n",
      "Epoch 213/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.7754 - acc: 0.7155 - val_loss: 0.7479 - val_acc: 0.7000\n",
      "Epoch 214/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 0.7964 - acc: 0.6897 - val_loss: 0.7456 - val_acc: 0.7000\n",
      "Epoch 215/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.7865 - acc: 0.6983 - val_loss: 0.7430 - val_acc: 0.7000\n",
      "Epoch 216/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8291 - acc: 0.6983 - val_loss: 0.7402 - val_acc: 0.7000\n",
      "Epoch 217/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.7349 - acc: 0.7241 - val_loss: 0.7369 - val_acc: 0.7000\n",
      "Epoch 218/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.7914 - acc: 0.7241 - val_loss: 0.7333 - val_acc: 0.7000\n",
      "Epoch 219/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8045 - acc: 0.7241 - val_loss: 0.7290 - val_acc: 0.7000\n",
      "Epoch 220/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8357 - acc: 0.6724 - val_loss: 0.7261 - val_acc: 0.7000\n",
      "Epoch 221/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 0.8242 - acc: 0.7414 - val_loss: 0.7231 - val_acc: 0.7000\n",
      "Epoch 222/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8878 - acc: 0.6983 - val_loss: 0.7206 - val_acc: 0.7000\n",
      "Epoch 223/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8091 - acc: 0.6724 - val_loss: 0.7188 - val_acc: 0.7000\n",
      "Epoch 224/1000\n",
      "116/116 [==============================] - 0s 32us/step - loss: 0.8607 - acc: 0.6638 - val_loss: 0.7167 - val_acc: 0.7000\n",
      "Epoch 225/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8293 - acc: 0.7155 - val_loss: 0.7149 - val_acc: 0.7000\n",
      "Epoch 226/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.7942 - acc: 0.7069 - val_loss: 0.7138 - val_acc: 0.7000\n",
      "Epoch 227/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 0.8079 - acc: 0.6897 - val_loss: 0.7127 - val_acc: 0.7000\n",
      "Epoch 228/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.8129 - acc: 0.7155 - val_loss: 0.7119 - val_acc: 0.7000\n",
      "Epoch 229/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.8366 - acc: 0.6724 - val_loss: 0.7116 - val_acc: 0.7000\n",
      "Epoch 230/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.7484 - acc: 0.7069 - val_loss: 0.7108 - val_acc: 0.7333\n",
      "Epoch 231/1000\n",
      "116/116 [==============================] - 0s 31us/step - loss: 0.8427 - acc: 0.7155 - val_loss: 0.7101 - val_acc: 0.7333\n",
      "Epoch 232/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.7798 - acc: 0.6983 - val_loss: 0.7105 - val_acc: 0.7333\n",
      "Epoch 233/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.7873 - acc: 0.6638 - val_loss: 0.7115 - val_acc: 0.7333\n",
      "Epoch 234/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.7121 - acc: 0.7586 - val_loss: 0.7125 - val_acc: 0.7333\n",
      "Epoch 235/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.7778 - acc: 0.7328 - val_loss: 0.7134 - val_acc: 0.7333\n",
      "Epoch 236/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.7502 - acc: 0.7069 - val_loss: 0.7143 - val_acc: 0.7333\n",
      "Epoch 237/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.7416 - acc: 0.7414 - val_loss: 0.7138 - val_acc: 0.7333\n",
      "Epoch 238/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.7356 - acc: 0.7069 - val_loss: 0.7134 - val_acc: 0.7333\n",
      "Epoch 239/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.7934 - acc: 0.6810 - val_loss: 0.7127 - val_acc: 0.7333\n",
      "Epoch 240/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.7918 - acc: 0.7155 - val_loss: 0.7120 - val_acc: 0.7333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 0.7235 - acc: 0.6897 - val_loss: 0.7116 - val_acc: 0.7333\n",
      "Epoch 00241: early stopping\n"
     ]
    }
   ],
   "source": [
    "# モデル学習\n",
    "epochs = 1000\n",
    "batch_size = len(X_train)    #1:確率的勾配降下法　M:（<=N）ミニバッチ勾配降下法\n",
    "# 過学習（オーバーフィッティング）防止の為、前のエポックのときと比べ誤差が増えたら学習を打ち切る\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "hist = model.fit(X_train, Y_train, epochs=epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_data=(X_validation, Y_validation),\n",
    "                 callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a5cfeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 学習の進み具合を可視化\n",
    "\n",
    "val_acc = hist.history['val_acc']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "plt.rc('font', family='serif')\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(val_acc)), val_acc, label='val_acc', color='red')\n",
    "plt.plot(range(len(val_loss)), val_loss, label='val_loss', color='black')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 102us/step\n",
      "[0.85606836950456777, 0.7297297313406661]\n"
     ]
    }
   ],
   "source": [
    "# 予測精度の評価\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print(loss_and_metrics) # 1番目：誤差関数の値、2番目：予測精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 4 0 4 1 4 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# テスト用データを使用して予測する\n",
    "test_size = 10\n",
    "X_predict = np.argmax(model.predict(X_test[0:test_size]),axis=1)\n",
    "print(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 4 2 4 1 4 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# テスト用データの正解　0：資産　1：負債　2：純資産　3：収益　4：費用\n",
    "print(np.argmax(Y_test[0:test_size],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False,  True,  True,  True, False,  True,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 予測が正しかったか判定する\n",
    "X_predict == np.argmax(Y_test[0:test_size],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"629pt\" viewBox=\"0.00 0.00 273.98 629.00\" width=\"274pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 625)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-625 269.9766,-625 269.9766,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 112063599560 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>112063599560</title>\n",
       "<polygon fill=\"none\" points=\"49.3691,-584.5 49.3691,-620.5 216.6074,-620.5 216.6074,-584.5 49.3691,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-598.3\">dense_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 112063599224 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>112063599224</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-511.5 80.8623,-547.5 185.1143,-547.5 185.1143,-511.5 80.8623,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-525.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 112063599560&#45;&gt;112063599224 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>112063599560-&gt;112063599224</title>\n",
       "<path d=\"M132.9883,-584.4551C132.9883,-576.3828 132.9883,-566.6764 132.9883,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-557.5903 132.9883,-547.5904 129.4884,-557.5904 136.4884,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112063666440 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>112063666440</title>\n",
       "<polygon fill=\"none\" points=\"0,-438.5 0,-474.5 265.9766,-474.5 265.9766,-438.5 0,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-452.3\">batch_normalization_1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 112063599224&#45;&gt;112063666440 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>112063599224-&gt;112063666440</title>\n",
       "<path d=\"M132.9883,-511.4551C132.9883,-503.3828 132.9883,-493.6764 132.9883,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-484.5903 132.9883,-474.5904 129.4884,-484.5904 136.4884,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112063665208 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>112063665208</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-365.5 57.5278,-401.5 208.4487,-401.5 208.4487,-365.5 57.5278,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-379.3\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 112063666440&#45;&gt;112063665208 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>112063666440-&gt;112063665208</title>\n",
       "<path d=\"M132.9883,-438.4551C132.9883,-430.3828 132.9883,-420.6764 132.9883,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-411.5903 132.9883,-401.5904 129.4884,-411.5904 136.4884,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112063600288 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>112063600288</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-292.5 80.8623,-328.5 185.1143,-328.5 185.1143,-292.5 80.8623,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-306.3\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 112063665208&#45;&gt;112063600288 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>112063665208-&gt;112063600288</title>\n",
       "<path d=\"M132.9883,-365.4551C132.9883,-357.3828 132.9883,-347.6764 132.9883,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-338.5903 132.9883,-328.5904 129.4884,-338.5904 136.4884,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112063927688 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>112063927688</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 265.9766,-255.5 265.9766,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-233.3\">batch_normalization_2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 112063600288&#45;&gt;112063927688 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>112063600288-&gt;112063927688</title>\n",
       "<path d=\"M132.9883,-292.4551C132.9883,-284.3828 132.9883,-274.6764 132.9883,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-265.5903 132.9883,-255.5904 129.4884,-265.5904 136.4884,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112064097696 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>112064097696</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-146.5 57.5278,-182.5 208.4487,-182.5 208.4487,-146.5 57.5278,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-160.3\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 112063927688&#45;&gt;112064097696 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>112063927688-&gt;112064097696</title>\n",
       "<path d=\"M132.9883,-219.4551C132.9883,-211.3828 132.9883,-201.6764 132.9883,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-192.5903 132.9883,-182.5904 129.4884,-192.5904 136.4884,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112063599168 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>112063599168</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-73.5 80.8623,-109.5 185.1143,-109.5 185.1143,-73.5 80.8623,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-87.3\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 112064097696&#45;&gt;112063599168 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>112064097696-&gt;112063599168</title>\n",
       "<path d=\"M132.9883,-146.4551C132.9883,-138.3828 132.9883,-128.6764 132.9883,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-119.5903 132.9883,-109.5904 129.4884,-119.5904 136.4884,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112063599056 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>112063599056</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-.5 57.5278,-36.5 208.4487,-36.5 208.4487,-.5 57.5278,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-14.3\">activation_3: Activation</text>\n",
       "</g>\n",
       "<!-- 112063599168&#45;&gt;112063599056 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>112063599168-&gt;112063599056</title>\n",
       "<path d=\"M132.9883,-73.4551C132.9883,-65.3828 132.9883,-55.6764 132.9883,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-46.5903 132.9883,-36.5904 129.4884,-46.5904 136.4884,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
