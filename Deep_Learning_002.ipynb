{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_file = open(\"Data/勘定科目コード.csv\",'r')\n",
    "account_data = account_file.readlines()\n",
    "account_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込んだデータをPythonリスト形式に変換して、先頭のラベルと2桁目以降の画像データを分離する\n",
    "n = len(account_data)\n",
    "account_list = []\n",
    "account_target = []\n",
    "for i in range(n):\n",
    "    account_list.append(list(map(int, account_data[i].split(',')[0])))\n",
    "    account_target.append(int(account_data[i].split(',')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用にデータを抽出する\n",
    "#N = 30000 # MNISTの一部のデータで実験\n",
    "# indices = np.random.permutation(range(n))[:N]\n",
    "#X = []\n",
    "#y = []\n",
    "#for i in indices:\n",
    "#    X.append(mnist_list[i])\n",
    "#    y.append(mnist_target[i])\n",
    "\n",
    "X = account_list\n",
    "y = account_target\n",
    "Y = np.eye(5)[y] # 1-of-k表現に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの正規化\n",
    "X = np.array(X)\n",
    "X = X / X.max()\n",
    "X = X - X.mean(axis=1).reshape(len(X), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "N_train = 0.8\n",
    "N_validation =0.2\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=N_train)\n",
    "\n",
    "# 訓練データをさらに訓練データと検証データに分類\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size = N_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 445\n",
      "Trainable params: 385\n",
      "Non-trainable params: 60\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# モデル設定\n",
    "n_in = len(X[0])    # 入力層のノード数\n",
    "n_hiddens = [10,10,10]   # 隠れ層のノード数\n",
    "n_out = len(Y[0])   # 出力層のノード数\n",
    "activation = 'relu' # 活性化関数　出力の値を0~1の確率変数に置き換える 'sigmoid''tanh''relu'　\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    return np.sqrt(2.0 / shape[0]) * np.random.normal(size=shape)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 入力層、隠れ層を生成する\n",
    "for i, input_dim in enumerate(([n_in] + n_hiddens)[:-1]):\n",
    "    model.add(Dense(n_hiddens[i], input_dim=input_dim,\n",
    "                    kernel_initializer=weight_variable))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(0.5)) # オーバーフィッティング対策　学習時に間引くノードの確率\n",
    "\n",
    "# 出力層を生成する  \n",
    "model.add(Dense(n_out, kernel_initializer=weight_variable))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 学習の準備　optimizer:勾配法　loss:誤差関数　metrics:評価関数\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.01, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 116 samples, validate on 30 samples\n",
      "Epoch 1/1000\n",
      "116/116 [==============================] - 2s 13ms/step - loss: 1.9749 - acc: 0.1724 - val_loss: 1.6220 - val_acc: 0.2667\n",
      "Epoch 2/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 2.0635 - acc: 0.1983 - val_loss: 1.6028 - val_acc: 0.3333\n",
      "Epoch 3/1000\n",
      "116/116 [==============================] - 0s 50us/step - loss: 1.6470 - acc: 0.2155 - val_loss: 1.5861 - val_acc: 0.4333\n",
      "Epoch 4/1000\n",
      "116/116 [==============================] - 0s 47us/step - loss: 2.0225 - acc: 0.1810 - val_loss: 1.5711 - val_acc: 0.4667\n",
      "Epoch 5/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.8809 - acc: 0.2672 - val_loss: 1.5566 - val_acc: 0.5000\n",
      "Epoch 6/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.7569 - acc: 0.2586 - val_loss: 1.5440 - val_acc: 0.5333\n",
      "Epoch 7/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.6728 - acc: 0.2931 - val_loss: 1.5330 - val_acc: 0.5333\n",
      "Epoch 8/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.5343 - acc: 0.2672 - val_loss: 1.5221 - val_acc: 0.5333\n",
      "Epoch 9/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.5742 - acc: 0.3448 - val_loss: 1.5114 - val_acc: 0.5333\n",
      "Epoch 10/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.6318 - acc: 0.3448 - val_loss: 1.5004 - val_acc: 0.5333\n",
      "Epoch 11/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.5715 - acc: 0.4138 - val_loss: 1.4910 - val_acc: 0.5333\n",
      "Epoch 12/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.5581 - acc: 0.4483 - val_loss: 1.4816 - val_acc: 0.5333\n",
      "Epoch 13/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.4631 - acc: 0.4224 - val_loss: 1.4724 - val_acc: 0.5667\n",
      "Epoch 14/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.4439 - acc: 0.4569 - val_loss: 1.4634 - val_acc: 0.6000\n",
      "Epoch 15/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.5109 - acc: 0.4138 - val_loss: 1.4548 - val_acc: 0.5667\n",
      "Epoch 16/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.3430 - acc: 0.5172 - val_loss: 1.4472 - val_acc: 0.5333\n",
      "Epoch 17/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.5477 - acc: 0.4655 - val_loss: 1.4400 - val_acc: 0.5333\n",
      "Epoch 18/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.4140 - acc: 0.5172 - val_loss: 1.4337 - val_acc: 0.5333\n",
      "Epoch 19/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.4590 - acc: 0.4828 - val_loss: 1.4276 - val_acc: 0.5333\n",
      "Epoch 20/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.3523 - acc: 0.4741 - val_loss: 1.4219 - val_acc: 0.5333\n",
      "Epoch 21/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.3974 - acc: 0.5431 - val_loss: 1.4165 - val_acc: 0.5333\n",
      "Epoch 22/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.4844 - acc: 0.5172 - val_loss: 1.4106 - val_acc: 0.5000\n",
      "Epoch 23/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.4005 - acc: 0.5345 - val_loss: 1.4052 - val_acc: 0.5000\n",
      "Epoch 24/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.3249 - acc: 0.5345 - val_loss: 1.3993 - val_acc: 0.5000\n",
      "Epoch 25/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.2664 - acc: 0.5603 - val_loss: 1.3928 - val_acc: 0.5000\n",
      "Epoch 26/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.3310 - acc: 0.5690 - val_loss: 1.3867 - val_acc: 0.5000\n",
      "Epoch 27/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.3323 - acc: 0.5345 - val_loss: 1.3805 - val_acc: 0.5000\n",
      "Epoch 28/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.2046 - acc: 0.6121 - val_loss: 1.3737 - val_acc: 0.5000\n",
      "Epoch 29/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.2886 - acc: 0.6121 - val_loss: 1.3675 - val_acc: 0.5000\n",
      "Epoch 30/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.3261 - acc: 0.5259 - val_loss: 1.3610 - val_acc: 0.5000\n",
      "Epoch 31/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.2896 - acc: 0.5690 - val_loss: 1.3549 - val_acc: 0.5000\n",
      "Epoch 32/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.2187 - acc: 0.5862 - val_loss: 1.3482 - val_acc: 0.5000\n",
      "Epoch 33/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.3041 - acc: 0.5603 - val_loss: 1.3416 - val_acc: 0.5000\n",
      "Epoch 34/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.2758 - acc: 0.5690 - val_loss: 1.3359 - val_acc: 0.5000\n",
      "Epoch 35/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.2868 - acc: 0.5690 - val_loss: 1.3306 - val_acc: 0.5000\n",
      "Epoch 36/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.3123 - acc: 0.5862 - val_loss: 1.3259 - val_acc: 0.5000\n",
      "Epoch 37/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 1.2385 - acc: 0.5776 - val_loss: 1.3218 - val_acc: 0.5000\n",
      "Epoch 38/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.2193 - acc: 0.5948 - val_loss: 1.3177 - val_acc: 0.5000\n",
      "Epoch 39/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.2065 - acc: 0.5776 - val_loss: 1.3131 - val_acc: 0.5000\n",
      "Epoch 40/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.3026 - acc: 0.5776 - val_loss: 1.3082 - val_acc: 0.5000\n",
      "Epoch 41/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.2719 - acc: 0.5948 - val_loss: 1.3024 - val_acc: 0.5000\n",
      "Epoch 42/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.2509 - acc: 0.5603 - val_loss: 1.2969 - val_acc: 0.5000\n",
      "Epoch 43/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.2943 - acc: 0.5603 - val_loss: 1.2907 - val_acc: 0.5000\n",
      "Epoch 44/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.2005 - acc: 0.5603 - val_loss: 1.2839 - val_acc: 0.5000\n",
      "Epoch 45/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 1.2973 - acc: 0.5776 - val_loss: 1.2785 - val_acc: 0.5000\n",
      "Epoch 46/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 1.1798 - acc: 0.5948 - val_loss: 1.2741 - val_acc: 0.5000\n",
      "Epoch 47/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.2922 - acc: 0.5603 - val_loss: 1.2701 - val_acc: 0.5000\n",
      "Epoch 48/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.1730 - acc: 0.5948 - val_loss: 1.2662 - val_acc: 0.5000\n",
      "Epoch 49/1000\n",
      "116/116 [==============================] - 0s 31us/step - loss: 1.2155 - acc: 0.5431 - val_loss: 1.2614 - val_acc: 0.5000\n",
      "Epoch 50/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 1.1866 - acc: 0.5862 - val_loss: 1.2567 - val_acc: 0.5000\n",
      "Epoch 51/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.2616 - acc: 0.5948 - val_loss: 1.2524 - val_acc: 0.5000\n",
      "Epoch 52/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 1.2576 - acc: 0.5862 - val_loss: 1.2483 - val_acc: 0.5000\n",
      "Epoch 53/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 1.2430 - acc: 0.5517 - val_loss: 1.2436 - val_acc: 0.5000\n",
      "Epoch 54/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.2011 - acc: 0.5690 - val_loss: 1.2390 - val_acc: 0.5000\n",
      "Epoch 55/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 1.1262 - acc: 0.5862 - val_loss: 1.2340 - val_acc: 0.5000\n",
      "Epoch 56/1000\n",
      "116/116 [==============================] - 0s 33us/step - loss: 1.1169 - acc: 0.5776 - val_loss: 1.2289 - val_acc: 0.5000\n",
      "Epoch 57/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.1755 - acc: 0.5948 - val_loss: 1.2230 - val_acc: 0.5000\n",
      "Epoch 58/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.1776 - acc: 0.5776 - val_loss: 1.2165 - val_acc: 0.5000\n",
      "Epoch 59/1000\n",
      "116/116 [==============================] - 0s 33us/step - loss: 1.1938 - acc: 0.5862 - val_loss: 1.2102 - val_acc: 0.5000\n",
      "Epoch 60/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 1.1901 - acc: 0.5948 - val_loss: 1.2038 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.2345 - acc: 0.5948 - val_loss: 1.1979 - val_acc: 0.5000\n",
      "Epoch 62/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 1.1478 - acc: 0.5948 - val_loss: 1.1919 - val_acc: 0.5000\n",
      "Epoch 63/1000\n",
      "116/116 [==============================] - 0s 33us/step - loss: 1.1860 - acc: 0.5862 - val_loss: 1.1870 - val_acc: 0.5000\n",
      "Epoch 64/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.2031 - acc: 0.5862 - val_loss: 1.1828 - val_acc: 0.5000\n",
      "Epoch 65/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.1556 - acc: 0.5948 - val_loss: 1.1775 - val_acc: 0.5000\n",
      "Epoch 66/1000\n",
      "116/116 [==============================] - 0s 33us/step - loss: 1.0757 - acc: 0.6207 - val_loss: 1.1714 - val_acc: 0.5000\n",
      "Epoch 67/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.1537 - acc: 0.5862 - val_loss: 1.1650 - val_acc: 0.5000\n",
      "Epoch 68/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.1490 - acc: 0.5776 - val_loss: 1.1585 - val_acc: 0.5000\n",
      "Epoch 69/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.0704 - acc: 0.5862 - val_loss: 1.1518 - val_acc: 0.5000\n",
      "Epoch 70/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 1.1182 - acc: 0.5948 - val_loss: 1.1453 - val_acc: 0.5000\n",
      "Epoch 71/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.1051 - acc: 0.5862 - val_loss: 1.1381 - val_acc: 0.5000\n",
      "Epoch 72/1000\n",
      "116/116 [==============================] - 0s 33us/step - loss: 1.2140 - acc: 0.6034 - val_loss: 1.1307 - val_acc: 0.5000\n",
      "Epoch 73/1000\n",
      "116/116 [==============================] - 0s 32us/step - loss: 1.0980 - acc: 0.6207 - val_loss: 1.1222 - val_acc: 0.5000\n",
      "Epoch 74/1000\n",
      "116/116 [==============================] - 0s 55us/step - loss: 1.1117 - acc: 0.5690 - val_loss: 1.1141 - val_acc: 0.5000\n",
      "Epoch 75/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.1334 - acc: 0.5776 - val_loss: 1.1056 - val_acc: 0.5000\n",
      "Epoch 76/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 1.1354 - acc: 0.6034 - val_loss: 1.0984 - val_acc: 0.5000\n",
      "Epoch 77/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.1897 - acc: 0.5862 - val_loss: 1.0921 - val_acc: 0.5000\n",
      "Epoch 78/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.1572 - acc: 0.6034 - val_loss: 1.0862 - val_acc: 0.5000\n",
      "Epoch 79/1000\n",
      "116/116 [==============================] - 0s 33us/step - loss: 1.0891 - acc: 0.6207 - val_loss: 1.0804 - val_acc: 0.5000\n",
      "Epoch 80/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.0942 - acc: 0.6034 - val_loss: 1.0760 - val_acc: 0.5000\n",
      "Epoch 81/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1759 - acc: 0.5776 - val_loss: 1.0700 - val_acc: 0.5333\n",
      "Epoch 82/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 1.1880 - acc: 0.5862 - val_loss: 1.0644 - val_acc: 0.5333\n",
      "Epoch 83/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.1188 - acc: 0.5862 - val_loss: 1.0602 - val_acc: 0.5333\n",
      "Epoch 84/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.0336 - acc: 0.6207 - val_loss: 1.0554 - val_acc: 0.5333\n",
      "Epoch 85/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 1.0776 - acc: 0.6034 - val_loss: 1.0506 - val_acc: 0.5333\n",
      "Epoch 86/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0365 - acc: 0.6121 - val_loss: 1.0460 - val_acc: 0.5333\n",
      "Epoch 87/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 1.1215 - acc: 0.6034 - val_loss: 1.0411 - val_acc: 0.5667\n",
      "Epoch 88/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 1.0779 - acc: 0.6034 - val_loss: 1.0353 - val_acc: 0.5667\n",
      "Epoch 89/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.0810 - acc: 0.6121 - val_loss: 1.0304 - val_acc: 0.5667\n",
      "Epoch 90/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0651 - acc: 0.6121 - val_loss: 1.0251 - val_acc: 0.6000\n",
      "Epoch 91/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 1.0281 - acc: 0.6379 - val_loss: 1.0207 - val_acc: 0.6000\n",
      "Epoch 92/1000\n",
      "116/116 [==============================] - 0s 48us/step - loss: 1.1329 - acc: 0.6121 - val_loss: 1.0150 - val_acc: 0.6000\n",
      "Epoch 93/1000\n",
      "116/116 [==============================] - 0s 49us/step - loss: 1.0902 - acc: 0.6207 - val_loss: 1.0106 - val_acc: 0.6000\n",
      "Epoch 94/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0749 - acc: 0.6121 - val_loss: 1.0065 - val_acc: 0.6000\n",
      "Epoch 95/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0428 - acc: 0.6034 - val_loss: 1.0027 - val_acc: 0.6000\n",
      "Epoch 96/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9988 - acc: 0.6466 - val_loss: 0.9981 - val_acc: 0.6000\n",
      "Epoch 97/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.9803 - acc: 0.6207 - val_loss: 0.9923 - val_acc: 0.6000\n",
      "Epoch 98/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.0690 - acc: 0.6207 - val_loss: 0.9872 - val_acc: 0.6000\n",
      "Epoch 99/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0238 - acc: 0.6121 - val_loss: 0.9844 - val_acc: 0.6000\n",
      "Epoch 100/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 1.0245 - acc: 0.6207 - val_loss: 0.9818 - val_acc: 0.6000\n",
      "Epoch 101/1000\n",
      "116/116 [==============================] - 0s 56us/step - loss: 0.9830 - acc: 0.6293 - val_loss: 0.9777 - val_acc: 0.6333\n",
      "Epoch 102/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.1298 - acc: 0.5690 - val_loss: 0.9735 - val_acc: 0.6667\n",
      "Epoch 103/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0333 - acc: 0.6034 - val_loss: 0.9698 - val_acc: 0.6667\n",
      "Epoch 104/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 1.0464 - acc: 0.6121 - val_loss: 0.9676 - val_acc: 0.6667\n",
      "Epoch 105/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9889 - acc: 0.6293 - val_loss: 0.9649 - val_acc: 0.6667\n",
      "Epoch 106/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 1.0685 - acc: 0.6121 - val_loss: 0.9632 - val_acc: 0.6667\n",
      "Epoch 107/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9696 - acc: 0.6466 - val_loss: 0.9611 - val_acc: 0.6667\n",
      "Epoch 108/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9538 - acc: 0.6207 - val_loss: 0.9589 - val_acc: 0.6667\n",
      "Epoch 109/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.1114 - acc: 0.5948 - val_loss: 0.9553 - val_acc: 0.6667\n",
      "Epoch 110/1000\n",
      "116/116 [==============================] - 0s 55us/step - loss: 0.9986 - acc: 0.5862 - val_loss: 0.9518 - val_acc: 0.6667\n",
      "Epoch 111/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.0284 - acc: 0.6466 - val_loss: 0.9498 - val_acc: 0.6667\n",
      "Epoch 112/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.1241 - acc: 0.6034 - val_loss: 0.9487 - val_acc: 0.6667\n",
      "Epoch 113/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 1.0224 - acc: 0.5862 - val_loss: 0.9470 - val_acc: 0.6667\n",
      "Epoch 114/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 1.0045 - acc: 0.6552 - val_loss: 0.9444 - val_acc: 0.6667\n",
      "Epoch 115/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 0.9420 - acc: 0.6207 - val_loss: 0.9402 - val_acc: 0.6667\n",
      "Epoch 116/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0079 - acc: 0.6207 - val_loss: 0.9364 - val_acc: 0.6667\n",
      "Epoch 117/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0284 - acc: 0.5862 - val_loss: 0.9314 - val_acc: 0.6667\n",
      "Epoch 118/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.9931 - acc: 0.6034 - val_loss: 0.9269 - val_acc: 0.6667\n",
      "Epoch 119/1000\n",
      "116/116 [==============================] - 0s 49us/step - loss: 0.9689 - acc: 0.6379 - val_loss: 0.9226 - val_acc: 0.6667\n",
      "Epoch 120/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 1.0507 - acc: 0.6379 - val_loss: 0.9196 - val_acc: 0.6667\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 43us/step - loss: 1.0327 - acc: 0.5948 - val_loss: 0.9171 - val_acc: 0.6667\n",
      "Epoch 122/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9916 - acc: 0.6034 - val_loss: 0.9150 - val_acc: 0.6667\n",
      "Epoch 123/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9821 - acc: 0.6121 - val_loss: 0.9121 - val_acc: 0.6667\n",
      "Epoch 124/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 1.0227 - acc: 0.6121 - val_loss: 0.9091 - val_acc: 0.6667\n",
      "Epoch 125/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.9928 - acc: 0.5776 - val_loss: 0.9053 - val_acc: 0.6667\n",
      "Epoch 126/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9871 - acc: 0.6121 - val_loss: 0.9016 - val_acc: 0.6667\n",
      "Epoch 127/1000\n",
      "116/116 [==============================] - 0s 57us/step - loss: 1.0237 - acc: 0.6121 - val_loss: 0.8985 - val_acc: 0.6667\n",
      "Epoch 128/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 0.9298 - acc: 0.6293 - val_loss: 0.8950 - val_acc: 0.6667\n",
      "Epoch 129/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9210 - acc: 0.6466 - val_loss: 0.8903 - val_acc: 0.6667\n",
      "Epoch 130/1000\n",
      "116/116 [==============================] - 0s 48us/step - loss: 0.9036 - acc: 0.6379 - val_loss: 0.8855 - val_acc: 0.6667\n",
      "Epoch 131/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9704 - acc: 0.6207 - val_loss: 0.8804 - val_acc: 0.6667\n",
      "Epoch 132/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9649 - acc: 0.6207 - val_loss: 0.8755 - val_acc: 0.6667\n",
      "Epoch 133/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.9644 - acc: 0.6034 - val_loss: 0.8697 - val_acc: 0.6667\n",
      "Epoch 134/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9830 - acc: 0.6121 - val_loss: 0.8636 - val_acc: 0.6667\n",
      "Epoch 135/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 1.0155 - acc: 0.6034 - val_loss: 0.8583 - val_acc: 0.6667\n",
      "Epoch 136/1000\n",
      "116/116 [==============================] - 0s 58us/step - loss: 0.9534 - acc: 0.6379 - val_loss: 0.8522 - val_acc: 0.7000\n",
      "Epoch 137/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9112 - acc: 0.6121 - val_loss: 0.8464 - val_acc: 0.7000\n",
      "Epoch 138/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9991 - acc: 0.6121 - val_loss: 0.8405 - val_acc: 0.7000\n",
      "Epoch 139/1000\n",
      "116/116 [==============================] - 0s 47us/step - loss: 0.8976 - acc: 0.6293 - val_loss: 0.8342 - val_acc: 0.7000\n",
      "Epoch 140/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.9648 - acc: 0.6034 - val_loss: 0.8285 - val_acc: 0.7000\n",
      "Epoch 141/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9136 - acc: 0.6293 - val_loss: 0.8229 - val_acc: 0.7000\n",
      "Epoch 142/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 1.0236 - acc: 0.5862 - val_loss: 0.8188 - val_acc: 0.7000\n",
      "Epoch 143/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9818 - acc: 0.6207 - val_loss: 0.8154 - val_acc: 0.7000\n",
      "Epoch 144/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9416 - acc: 0.6293 - val_loss: 0.8117 - val_acc: 0.7000\n",
      "Epoch 145/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.9413 - acc: 0.6121 - val_loss: 0.8085 - val_acc: 0.7000\n",
      "Epoch 146/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9740 - acc: 0.6121 - val_loss: 0.8057 - val_acc: 0.7000\n",
      "Epoch 147/1000\n",
      "116/116 [==============================] - 0s 57us/step - loss: 0.8680 - acc: 0.6638 - val_loss: 0.8038 - val_acc: 0.7000\n",
      "Epoch 148/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.9058 - acc: 0.6293 - val_loss: 0.8024 - val_acc: 0.7000\n",
      "Epoch 149/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.9120 - acc: 0.6293 - val_loss: 0.8003 - val_acc: 0.7000\n",
      "Epoch 150/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9545 - acc: 0.6207 - val_loss: 0.7973 - val_acc: 0.7000\n",
      "Epoch 151/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9270 - acc: 0.6293 - val_loss: 0.7939 - val_acc: 0.7000\n",
      "Epoch 152/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.9155 - acc: 0.6121 - val_loss: 0.7907 - val_acc: 0.7000\n",
      "Epoch 153/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9050 - acc: 0.6207 - val_loss: 0.7877 - val_acc: 0.7000\n",
      "Epoch 154/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9792 - acc: 0.6034 - val_loss: 0.7860 - val_acc: 0.7000\n",
      "Epoch 155/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9520 - acc: 0.6379 - val_loss: 0.7859 - val_acc: 0.7000\n",
      "Epoch 156/1000\n",
      "116/116 [==============================] - 0s 53us/step - loss: 0.9638 - acc: 0.6034 - val_loss: 0.7859 - val_acc: 0.7000\n",
      "Epoch 157/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9788 - acc: 0.6466 - val_loss: 0.7864 - val_acc: 0.7000\n",
      "Epoch 158/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.9801 - acc: 0.6379 - val_loss: 0.7849 - val_acc: 0.7000\n",
      "Epoch 159/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8418 - acc: 0.6552 - val_loss: 0.7838 - val_acc: 0.7000\n",
      "Epoch 160/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9437 - acc: 0.6207 - val_loss: 0.7825 - val_acc: 0.7000\n",
      "Epoch 161/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9277 - acc: 0.6466 - val_loss: 0.7801 - val_acc: 0.7000\n",
      "Epoch 162/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9615 - acc: 0.6466 - val_loss: 0.7775 - val_acc: 0.7000\n",
      "Epoch 163/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8377 - acc: 0.6638 - val_loss: 0.7741 - val_acc: 0.7000\n",
      "Epoch 164/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8904 - acc: 0.6379 - val_loss: 0.7712 - val_acc: 0.7333\n",
      "Epoch 165/1000\n",
      "116/116 [==============================] - 0s 55us/step - loss: 0.8891 - acc: 0.6466 - val_loss: 0.7701 - val_acc: 0.7333\n",
      "Epoch 166/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8963 - acc: 0.6466 - val_loss: 0.7683 - val_acc: 0.7333\n",
      "Epoch 167/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8152 - acc: 0.6724 - val_loss: 0.7679 - val_acc: 0.7333\n",
      "Epoch 168/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8591 - acc: 0.6379 - val_loss: 0.7662 - val_acc: 0.7333\n",
      "Epoch 169/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.9152 - acc: 0.6552 - val_loss: 0.7652 - val_acc: 0.7333\n",
      "Epoch 170/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9062 - acc: 0.6293 - val_loss: 0.7626 - val_acc: 0.7667\n",
      "Epoch 171/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9048 - acc: 0.6379 - val_loss: 0.7604 - val_acc: 0.7667\n",
      "Epoch 172/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8580 - acc: 0.6293 - val_loss: 0.7582 - val_acc: 0.7667\n",
      "Epoch 173/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9158 - acc: 0.5948 - val_loss: 0.7564 - val_acc: 0.7667\n",
      "Epoch 174/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9140 - acc: 0.6466 - val_loss: 0.7538 - val_acc: 0.7667\n",
      "Epoch 175/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8942 - acc: 0.6379 - val_loss: 0.7498 - val_acc: 0.7667\n",
      "Epoch 176/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8936 - acc: 0.6466 - val_loss: 0.7454 - val_acc: 0.7667\n",
      "Epoch 177/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.8470 - acc: 0.6121 - val_loss: 0.7415 - val_acc: 0.7667\n",
      "Epoch 178/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.9222 - acc: 0.6293 - val_loss: 0.7372 - val_acc: 0.7667\n",
      "Epoch 179/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8196 - acc: 0.6293 - val_loss: 0.7341 - val_acc: 0.7667\n",
      "Epoch 180/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8514 - acc: 0.6207 - val_loss: 0.7301 - val_acc: 0.7667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8442 - acc: 0.6121 - val_loss: 0.7271 - val_acc: 0.7667\n",
      "Epoch 182/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.9264 - acc: 0.6293 - val_loss: 0.7260 - val_acc: 0.7333\n",
      "Epoch 183/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8689 - acc: 0.6466 - val_loss: 0.7243 - val_acc: 0.7333\n",
      "Epoch 184/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8996 - acc: 0.6207 - val_loss: 0.7228 - val_acc: 0.7333\n",
      "Epoch 185/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.8838 - acc: 0.6379 - val_loss: 0.7211 - val_acc: 0.7333\n",
      "Epoch 186/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9230 - acc: 0.6379 - val_loss: 0.7190 - val_acc: 0.7333\n",
      "Epoch 187/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8876 - acc: 0.6293 - val_loss: 0.7174 - val_acc: 0.7333\n",
      "Epoch 188/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9543 - acc: 0.6121 - val_loss: 0.7170 - val_acc: 0.7333\n",
      "Epoch 189/1000\n",
      "116/116 [==============================] - 0s 57us/step - loss: 0.9024 - acc: 0.6293 - val_loss: 0.7167 - val_acc: 0.7333\n",
      "Epoch 190/1000\n",
      "116/116 [==============================] - 0s 46us/step - loss: 0.7622 - acc: 0.6897 - val_loss: 0.7165 - val_acc: 0.7333\n",
      "Epoch 191/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8123 - acc: 0.6552 - val_loss: 0.7173 - val_acc: 0.7333\n",
      "Epoch 192/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8499 - acc: 0.6293 - val_loss: 0.7158 - val_acc: 0.7333\n",
      "Epoch 193/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8406 - acc: 0.6552 - val_loss: 0.7162 - val_acc: 0.7333\n",
      "Epoch 194/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.8238 - acc: 0.6552 - val_loss: 0.7165 - val_acc: 0.7333\n",
      "Epoch 195/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8750 - acc: 0.6121 - val_loss: 0.7160 - val_acc: 0.7333\n",
      "Epoch 196/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8672 - acc: 0.6724 - val_loss: 0.7151 - val_acc: 0.7333\n",
      "Epoch 197/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8836 - acc: 0.6379 - val_loss: 0.7140 - val_acc: 0.7333\n",
      "Epoch 198/1000\n",
      "116/116 [==============================] - 0s 51us/step - loss: 0.9273 - acc: 0.6207 - val_loss: 0.7129 - val_acc: 0.7333\n",
      "Epoch 199/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.8411 - acc: 0.6466 - val_loss: 0.7121 - val_acc: 0.7333\n",
      "Epoch 200/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9090 - acc: 0.6552 - val_loss: 0.7120 - val_acc: 0.7333\n",
      "Epoch 201/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8923 - acc: 0.6293 - val_loss: 0.7113 - val_acc: 0.7667\n",
      "Epoch 202/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8379 - acc: 0.6293 - val_loss: 0.7095 - val_acc: 0.7667\n",
      "Epoch 203/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.9211 - acc: 0.6207 - val_loss: 0.7086 - val_acc: 0.7333\n",
      "Epoch 204/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.8456 - acc: 0.6552 - val_loss: 0.7065 - val_acc: 0.7333\n",
      "Epoch 205/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8405 - acc: 0.6293 - val_loss: 0.7038 - val_acc: 0.7333\n",
      "Epoch 206/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8369 - acc: 0.6552 - val_loss: 0.7000 - val_acc: 0.7333\n",
      "Epoch 207/1000\n",
      "116/116 [==============================] - 0s 53us/step - loss: 0.8649 - acc: 0.6379 - val_loss: 0.6977 - val_acc: 0.7000\n",
      "Epoch 208/1000\n",
      "116/116 [==============================] - 0s 45us/step - loss: 0.7912 - acc: 0.6466 - val_loss: 0.6950 - val_acc: 0.7000\n",
      "Epoch 209/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.8908 - acc: 0.6379 - val_loss: 0.6932 - val_acc: 0.7000\n",
      "Epoch 210/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8715 - acc: 0.6379 - val_loss: 0.6913 - val_acc: 0.7000\n",
      "Epoch 211/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.8929 - acc: 0.6379 - val_loss: 0.6901 - val_acc: 0.7000\n",
      "Epoch 212/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.9493 - acc: 0.6034 - val_loss: 0.6890 - val_acc: 0.7000\n",
      "Epoch 213/1000\n",
      "116/116 [==============================] - 0s 42us/step - loss: 0.9074 - acc: 0.6638 - val_loss: 0.6872 - val_acc: 0.7000\n",
      "Epoch 214/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.8289 - acc: 0.6552 - val_loss: 0.6837 - val_acc: 0.7000\n",
      "Epoch 215/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.7624 - acc: 0.6810 - val_loss: 0.6807 - val_acc: 0.7000\n",
      "Epoch 216/1000\n",
      "116/116 [==============================] - 0s 52us/step - loss: 0.7848 - acc: 0.6724 - val_loss: 0.6773 - val_acc: 0.7000\n",
      "Epoch 217/1000\n",
      "116/116 [==============================] - 0s 48us/step - loss: 0.8091 - acc: 0.6810 - val_loss: 0.6743 - val_acc: 0.7000\n",
      "Epoch 218/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8161 - acc: 0.6983 - val_loss: 0.6717 - val_acc: 0.7000\n",
      "Epoch 219/1000\n",
      "116/116 [==============================] - 0s 40us/step - loss: 0.8236 - acc: 0.6379 - val_loss: 0.6684 - val_acc: 0.7000\n",
      "Epoch 220/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.7862 - acc: 0.6724 - val_loss: 0.6662 - val_acc: 0.7000\n",
      "Epoch 221/1000\n",
      "116/116 [==============================] - 0s 32us/step - loss: 0.8312 - acc: 0.6638 - val_loss: 0.6653 - val_acc: 0.7000\n",
      "Epoch 222/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8033 - acc: 0.6724 - val_loss: 0.6639 - val_acc: 0.7000\n",
      "Epoch 223/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.7903 - acc: 0.6724 - val_loss: 0.6613 - val_acc: 0.7000\n",
      "Epoch 224/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.7568 - acc: 0.6810 - val_loss: 0.6573 - val_acc: 0.7000\n",
      "Epoch 225/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.7863 - acc: 0.6724 - val_loss: 0.6540 - val_acc: 0.7000\n",
      "Epoch 226/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.8009 - acc: 0.6810 - val_loss: 0.6517 - val_acc: 0.7000\n",
      "Epoch 227/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.7891 - acc: 0.6897 - val_loss: 0.6507 - val_acc: 0.7333\n",
      "Epoch 228/1000\n",
      "116/116 [==============================] - 0s 33us/step - loss: 0.7723 - acc: 0.6466 - val_loss: 0.6505 - val_acc: 0.7333\n",
      "Epoch 229/1000\n",
      "116/116 [==============================] - 0s 54us/step - loss: 0.7468 - acc: 0.6983 - val_loss: 0.6499 - val_acc: 0.7333\n",
      "Epoch 230/1000\n",
      "116/116 [==============================] - 0s 43us/step - loss: 0.8306 - acc: 0.6207 - val_loss: 0.6493 - val_acc: 0.7333\n",
      "Epoch 231/1000\n",
      "116/116 [==============================] - 0s 34us/step - loss: 0.8331 - acc: 0.6810 - val_loss: 0.6477 - val_acc: 0.7333\n",
      "Epoch 232/1000\n",
      "116/116 [==============================] - 0s 44us/step - loss: 0.6821 - acc: 0.7069 - val_loss: 0.6466 - val_acc: 0.7333\n",
      "Epoch 233/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8128 - acc: 0.6638 - val_loss: 0.6442 - val_acc: 0.7333\n",
      "Epoch 234/1000\n",
      "116/116 [==============================] - 0s 33us/step - loss: 0.7856 - acc: 0.7414 - val_loss: 0.6412 - val_acc: 0.7333\n",
      "Epoch 235/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.8679 - acc: 0.6897 - val_loss: 0.6388 - val_acc: 0.7333\n",
      "Epoch 236/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8173 - acc: 0.6983 - val_loss: 0.6348 - val_acc: 0.7333\n",
      "Epoch 237/1000\n",
      "116/116 [==============================] - 0s 32us/step - loss: 0.7665 - acc: 0.6638 - val_loss: 0.6306 - val_acc: 0.7333\n",
      "Epoch 238/1000\n",
      "116/116 [==============================] - 0s 33us/step - loss: 0.8296 - acc: 0.6810 - val_loss: 0.6265 - val_acc: 0.7333\n",
      "Epoch 239/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.7758 - acc: 0.6552 - val_loss: 0.6231 - val_acc: 0.7333\n",
      "Epoch 240/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8107 - acc: 0.7069 - val_loss: 0.6202 - val_acc: 0.7333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.8218 - acc: 0.6897 - val_loss: 0.6180 - val_acc: 0.7333\n",
      "Epoch 242/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.8221 - acc: 0.6810 - val_loss: 0.6150 - val_acc: 0.7333\n",
      "Epoch 243/1000\n",
      "116/116 [==============================] - 0s 38us/step - loss: 0.8481 - acc: 0.6810 - val_loss: 0.6114 - val_acc: 0.7333\n",
      "Epoch 244/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.7929 - acc: 0.6552 - val_loss: 0.6087 - val_acc: 0.7333\n",
      "Epoch 245/1000\n",
      "116/116 [==============================] - 0s 33us/step - loss: 0.8735 - acc: 0.6810 - val_loss: 0.6071 - val_acc: 0.7333\n",
      "Epoch 246/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.9286 - acc: 0.6810 - val_loss: 0.6058 - val_acc: 0.7333\n",
      "Epoch 247/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.8919 - acc: 0.6897 - val_loss: 0.6067 - val_acc: 0.7333\n",
      "Epoch 248/1000\n",
      "116/116 [==============================] - 0s 31us/step - loss: 0.7904 - acc: 0.6983 - val_loss: 0.6071 - val_acc: 0.7333\n",
      "Epoch 249/1000\n",
      "116/116 [==============================] - 0s 53us/step - loss: 0.8742 - acc: 0.6638 - val_loss: 0.6070 - val_acc: 0.7333\n",
      "Epoch 250/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.7557 - acc: 0.6466 - val_loss: 0.6071 - val_acc: 0.7333\n",
      "Epoch 251/1000\n",
      "116/116 [==============================] - 0s 36us/step - loss: 0.7656 - acc: 0.7414 - val_loss: 0.6081 - val_acc: 0.7333\n",
      "Epoch 252/1000\n",
      "116/116 [==============================] - 0s 41us/step - loss: 0.7240 - acc: 0.7069 - val_loss: 0.6082 - val_acc: 0.7333\n",
      "Epoch 253/1000\n",
      "116/116 [==============================] - 0s 37us/step - loss: 0.7426 - acc: 0.7414 - val_loss: 0.6088 - val_acc: 0.7333\n",
      "Epoch 254/1000\n",
      "116/116 [==============================] - 0s 30us/step - loss: 0.8693 - acc: 0.6724 - val_loss: 0.6100 - val_acc: 0.7333\n",
      "Epoch 255/1000\n",
      "116/116 [==============================] - 0s 35us/step - loss: 0.8420 - acc: 0.6552 - val_loss: 0.6118 - val_acc: 0.7333\n",
      "Epoch 256/1000\n",
      "116/116 [==============================] - 0s 39us/step - loss: 0.8092 - acc: 0.6552 - val_loss: 0.6149 - val_acc: 0.7333\n",
      "Epoch 00256: early stopping\n"
     ]
    }
   ],
   "source": [
    "# モデル学習\n",
    "epochs = 1000\n",
    "batch_size = len(X_train)    #1:確率的勾配降下法　M:（<=N）ミニバッチ勾配降下法\n",
    "# 過学習（オーバーフィッティング）防止の為、前のエポックのときと比べ誤差が増えたら学習を打ち切る\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "hist = model.fit(X_train, Y_train, epochs=epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_data=(X_validation, Y_validation),\n",
    "                 callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3Xucj3X+//HHa2YcZiw5DRIpIhmnMuNsxNC5lRRbm+VrWhK12N1C1mpp0m4laW1Rkq3UL4RyaI3D6MDkFOVMmRQyiggJ8/798fnQOIwZM5+Zaz6fed5vt7nN53N9Ltf1ejfT0+V9va/325xziIhIaAnzugAREQk8hbuISAhSuIuIhCCFu4hICFK4i4iEIIW7iEgIUriLiIQghbuISAhSuIuIhKAIr05csWJFd8UVV3h1ehGRoLRq1ap9zrno7PbzLNyvuOIKVq5c6dXpRUSCkpml5WQ/dcuIiIQghbuISAhSuIuIhCCFu4hICFK4i4iEIIW7iEgIUriLiISgoAv3b7/9lkGDBrFv3z6vSxERKbRyFO5mVsXMXjazFRfYp4eZDTKzJ8zs/cCVeKYDBw4wZswYXn755fw6hYhI0MvplXtrYBZg5/vQzFoDNZxzzzrnHgOGBqi+c8TExNC+fXvGjx/PiRMn8us0IiJBLUfh7pybBhy6wC73AWFm9iczSwLCA1FcVh5++GF27tzJzJkz8/M0IiJBK1B97jWAy51zY4GngXfNrNzZO5lZbzNbaWYr09PTc32y2267jVq1avHkk0/inMt91SIiISpQ4X4QSAVwzv0A7AEanb2Tc26Ccy7WORcbHZ3tpGZZCg8PZ+jQoaxevZr58+fn+jgiIqEq1+FuZqXM7FRCLwRq+reHAVWAL/NeXta6d+9OjRo1+Nvf/kZGRkZ+nkpEJOjkdLRMW6A7cKmZDTOzSKAnMNK/y2SguJkNA/4NjHDOfR34cn9VrFgxRo0axapVq5g6dWp+nkpEJOiYV33WsbGxLq/zuWdkZBAXF8e+ffvYtGkTkZGRAapORKRwMrNVzrnY7PYLuoeYMgsLC+Ppp5/m66+/5vnnn/e6HBGRQiOowx2gXbt23H777SQlJbFr1y6vyxERKRSCPtwBnnnmGX755Rf69eunoZEiIoRIuNeuXZvHH3+cmTNn8vrrr3tdjoiI50Ii3AEGDRpEfHw8DzzwABs3bvS6HBERT4VMuEdERDB16lRKlSpF165dOXLkiNcliYh4JmTCHaBq1aq8/vrrrF+/noceesjrckREPBNS4Q5www03MHToUCZNmsSUKVO8LkdExBMhF+4AI0aMID4+nr59+6r/XUSKpJAM98z973fffTcHDx70uiQRkQIVkuEOvv73qVOnsnnzZrp06cIvv/zidUkiIgUmZMMdICEhgYkTJ5KcnMz999+vB5xEpMiI8LqA/NazZ0927tzJ8OHDqVatGklJSV6XJCKS70I+3AGGDRvGzp07efLJJ6levTp9+/b1uiQRkXxVJMLdzBg/fjy7d++mf//+VK1alU6dOnldlohIvgnpPvfMIiIieOutt4iNjaVr167MnTvX65JERPJNkQl3gFKlSjFv3jzq169P586dmTNnjtcliYjkiyIV7gDly5cnOTmZBg0a0LlzZ1577TWvSxIRCbgiF+4A5cqVIzk5mTZt2tCzZ0/+8pe/cPLkSa/LEhEJmCIZ7gBly5Zl/vz59OvXj2eeeYYbb7xRKzmJSMgosuEOUKxYMV544QVeeeUVli1bRsOGDZk9e7bXZYmI5Fm24W5mVczsZTNbkc1+bc3shJnVD1x5BaNXr16sWrWK6tWr06lTJxITEzUfjYgEtZxcubcGZgGW1Q5mVgnoBnwToLoKXN26dVm+fDlDhgxh8uTJNGjQgEWLFnldlohIrmQb7s65acChrD43szAgCXgsgHV5okSJEiQlJfHxxx9TsmRJEhISGDRokCYdE5GgE4g+98HAROfc/gAcq1Bo3rw5a9asoX///owZM4b4+HjS0tK8LktEJMfyFO5mVhKoD7Qzs8HAJUCimSVksX9vM1tpZivT09Pzcup8FxUVxbhx45g2bRobN26kcePGutkqIkEjV+FuZqXMLNo597Nz7l7n3Gjn3GjgR+AV59zC8/0559wE51yscy42Ojo6L3UXmC5durB69Wpq1qxJp06dGDJkiMbEi0ihl5PRMm2B7sClZjbMzCKBnsDITPsUM7Nh+K7ce5tZvXyq1xO1atXik08+oU+fPowePZqbb76Zffv2eV2WiEiWzKsFLGJjY93KlSs9OXdeTJo0iQcffJDKlSszY8YMmjRp4nVJIlKEmNkq51xsdvsV6YeYcqNXr1589NFHALRq1YpJkyZ5XJGIyLkU7rkQGxvLqlWraNOmDYmJifTp04djx455XZaIyGkK91yqWLEi8+fPZ/DgwUyYMIH4+Hi+/vprr8sSEQEU7nkSHh7Ok08+yYwZM04Pl3z//fe9LktEROEeCJ07d2b16tXUqFGD22+/nYceekhz04iIpxTuAXLVVVexbNkyHn74Yf79739z9dVX8+abb5KRkeF1aSJSBCncA6hkyZKMHTuW1NRULrvsMn7/+98TFxfHvHnz8GrIqYgUTQr3fBAXF0dqaipTpkxh//793HLLLcTHx7N8+XKvSxORIkLhnk/Cw8Pp3r07mzZtYvz48Wzfvp2WLVsyYMAADh8+7HV5IhLiFO75rHjx4vTt25fNmzfz4IMPMnbsWBo0aMCyZcu8Lk1EQpjCvYCULl2aF154gaVLl+Kco02bNowcOVKTkIlIvlC4F7A2bdrw2Wef0a1bN4YPH85tt93GgQMHvC5LREKMwt0Dl1xyCW+88QYTJkwgOTmZRo0aMXfuXI2oEZGAUbh76I9//CMffvghkZGR3HrrrcTHx7NkyRKvyxKREKBw91jz5s1Zt24d48eP58svv6Rdu3a0atVKIS8ieaJwLwROjajZtm0b48aNY+fOnbRr147OnTuzdetWr8sTkSCkcC9EIiMj6d+/P5s3byYpKYnk5GRiYmIYOHCgVn4SkYuicC+EIiMjGTJkCNu2baNnz548//zz1KpVi1GjRukBKBHJEYV7IVa5cmUmTJjA559/Trt27fjb3/5GvXr1mDFjhkbWiMgFKdyDQL169Zg5cyYpKSlccskldOnShZtuuoktW7Z4XZqIFFIK9yASHx/P6tWrGTt2LMuXL6dRo0ZMnjzZ67JEpBBSuAeZiIgIHn74YTZv3kzLli35v//7PxITEzl69KjXpYlIIZKjcDezKmb2spmtyOLznmb2opn91cymmlnLwJYpZ6tSpQr/+9//GDZsGJMmTaJFixYaNikip+X0yr01MAuwLD6/DBjgnPsX8BzwUgBqk2yEh4czcuRI5s6dy86dO7nuuuuYOnWq12WJSCGQo3B3zk0DDl3g8yeccz9nOuZPAahNcujmm29mzZo1NGzYkHvvvZfevXtz5MgRr8sSEQ8FtM/dzAz4EzAoi897m9lKM1uZnp4eyFMXeZdffjlLlixhyJAhvPzyyzRt2pTU1FSvyxIRjwQs3P3B/i9gsnPuvCtROOcmOOdinXOx0dHRgTq1+BUrVoykpCTmz5/PgQMHaNGiBf379+fHH3/0ujQRKWC5DnczK2Vm0f7X4cBY4D3n3Hwz6xKoAuXi3XDDDWzYsIGHHnqI8ePHU69ePaZPn64Hn0SKkJyOlmkLdAcuNbNhZhYJ9ARG+nf5F3AH8LiZLcF3U1U8VKZMGcaOHUtqaiqVKlXirrvu4s4779TCICJFhHl1NRcbG+tWrlzpybmLmhMnTjBmzBiGDh3KlVdeycyZM6lXr57XZYlILpjZKudcbHb76SGmIiAiIoK//vWvLF68mIMHD9KsWTNmzJjhdVkiko8U7kVI69atWbVqFTExMXTp0oXHHntMC3SLhCiFexFz2WWXkZKSwv33309SUhKdO3fWmHiREKRwL4JKlCjBxIkTeeGFF3j//fdJSEjg+++/97osEQkghXsR1q9fP6ZNm8aaNWto3bo1u3fv9rokEQkQhXsRd+edd7JgwQK++eYbEhIS+O6777wuSUQCQOEutGnThjlz5pCWlkaHDh20XqtICFC4C+BbCOS9995j27ZtdOjQQX3wIkFO4S6ntW/fntmzZ7Np0yZat27Ntm3bvC5JRHJJ4S5n6NixIx988AHp6ek0bdqURYsWeV2SiOSCwl3O0bZtWz799FOqVq1Kx44d+fvf/86JEye8LktELoLCXc6rZs2aLFu2jO7du/OPf/yDNm3a8MUXX3hdlojkkMJdslS6dGkmT57MW2+9xaZNm2jYsCEPP/ywFuMWCQIKd8lWt27d2L59O/369WPcuHHExsaydu1ar8sSkQtQuEuOlC9fnnHjxvHBBx+wf/9+mjRpwoMPPqiHnkQKKYW7XJQbbriBzz//nL59+zJx4kRq1arFU089xfHjx70uTUQyUbjLRatQoQLjxo1jw4YNJCQkMHjwYBo3bszSpUu9Lk1E/BTukmu1a9dm1qxZzJ49m8OHD9O2bVt69uzJ3r17vS5NpMhTuEue3X777WzYsIEhQ4bw5ptvUrduXV566SUyMjK8Lk2kyFK4S0BERUWRlJTE2rVradSoEQ888ADx8fF8/vnnXpcmUiQp3CWgrrnmGhYtWsTkyZPZsGEDjRo1ok+fPhw6dMjr0kSKlByFu5lVMbOXzWxFFp+HmdloMxtmZhPNrHlgy5RgYmb06NGDbdu2MWDAACZOnEhMTAyTJk3SNAYiBSSnV+6tgVmAZfF5V6CMc24U8CgwxczCA1CfBLHy5cvz7LPP8uGHH1KlShUSExOJiYnh1Vdf5ZdffvG6PJGQlqNwd85NAy707+pbgWX+fX8AfgZi8lydhIRWrVqRmprKu+++S2RkJL169aJ+/frMmzfP69JEQlag+twrcWb4H/RvO4OZ9TazlWa2Mj09PUCnlmBgZtxxxx2sWbOG9957j7CwMG655RZ69uzJ/v37vS5PJOQEKtz3AqUzvS/j33YG59wE51yscy42Ojo6QKeWYGJm3Hbbbaxdu5bHHnuM119/nTp16tCvXz8WL16s4ZMiAZLrcDezUmZ2KqHnAC3828sDJYH1eS9PQlWJEiUYNWoUqamptG3blldffZX27dvToEEDpk6dysmTJ70uUSSo5XS0TFugO3Cpf0RMJNATGOnf5f8Bh8zs78C/gD845/R/p2SrSZMmTJs2jfT0dP773/9iZtx7773Uq1ePl19+mT179nhdokhQMuecJyeOjY11K1eu9OTcUnhlZGTw7rvvMnLkyNPTCtevX58OHTrQsWNH4uPj+c1vfuNxlSLeMbNVzrnYbPdTuEth5Jxj1apVLFy4kOTkZD788EOOHTtGREQE8fHx9O3blzvuuIOIiAivSxUpUAp3CSlHjx7lk08+ITk5mbfffpuvvvqK6tWr06lTJ+Li4qhfvz7XXHMNkZGRXpcqkq8U7hKyTp48yZw5c3jppZdISUnh8OHDgO8m7Y033sjQoUNp1qyZx1WK5I+chrvmlpGgEx4ezm9/+1vmzJnDgQMH2LhxI++88w59+vRh2bJlNG/enC5durBp0yavSxXxjMJdglpERAR169blrrvuYuzYsWzfvp3HH3+c//3vf9SvX58//vGPfPvtt16XKVLgFO4SUkqXLs3w4cNPL+j92muvcdVVV/HnP/+ZL774Aq+6IUUKmsJdQlKlSpUYO3YsmzdvpmvXrjz33HM0aNCAevXq8Ze//EVPw0rIU7hLSLvyyit57bXX2LVrF+PHj6dq1aqMGzeO9u3bU6NGDQYPHsyOHTu8LlMk4BTuUiRUrlyZvn37snDhQvbv389bb71F48aNefrpp6lVqxZ33nknS5YsUbeNhAyFuxQ5UVFRdOvWjffee48dO3bw6KOPsnTpUtq1a0fjxo2ZMmWKumwk6CncpUirVq0aSUlJ7Ny5k4kTJwLQo0cPWrRogZ7DkGCmcBcBIiMjuf/++/nss8+YMmUKaWlpNG3alN69e7Nv3z6vyxO5aAp3kUzMjO7du7NlyxYGDhzIpEmTqFOnDpMmTVJ/vAQVhbvIeZQpU4ZnnnmGtWvX0qBBAxITE7n77rv54YcfvC5NJEcU7iIXEBMTw+LFi3nqqaeYNWsWjRo1YsmSJV6XJZIthbtINsLCwnjkkUdYvnw5UVFRtG/fniFDhnD8+HGvSxPJksJdJIeaNGnC6tWrSUxMZPTo0bRs2ZKtW7d6XZbIeSncRS5CqVKlmDhxItOmTWP79u1ce+21utkqhZLCXSQXunTpwrp162jatCmJiYl069aN/fv3e12WyGkKd5FcqlatGgsWLGD06NG8++67NGrUiJSUFK/LEgEU7iJ5Eh4ezqOPPsonn3xCiRIlaNeuHffdd9/pxb1FvJKjcDezDmY23sxGmNnfz/P5lWY23cwGm9lbZvbbwJcqUnjFxcWxZs0a/vznPzNr1iwaN25Mx44dmTlzJidOnPC6PCmCsl1D1cyigHVAjHPumJlNB8Y75xZm2uc/wBbn3Bgzuxb4f8652hc6rtZQlVC1f/9+XnzxRcaPH88333xD9erVueeee7jlllto3rw5JUqU8LpECWKBXEO1BZDmnDvmf/8xcOtZ+3wHRPtfRwOrclqoSKgpV64cQ4YM4auvvuLdd9+lQYMGPPPMM1x//fWULVuWDh068MQTT5CamqpRNpJvcnLlfg/QzTl3h//9/cD1zrn7Mu1TBngXWAs0BUY65z44z7F6A70BLr/88iZpaWmBaodIoXbgwAGWLl3K4sWLWbRoEevWrQOgTp06JCYm0qNHDypXruxxlRIMcnrlnpNwTwCGOucS/O8HAdWcc4My7TMDeMc5N9XMooGtQE3nXJYTcahbRoqyffv28f777/PKK6/w0UcfERERQXx8PDExMURFRVGyZElKlixJZGQklSpVonbt2lx99dWULl3a69LFY4EM9/P2uQNrgBPOuYNmtgL4q3NuiZlFAHuBOs65LOdKVbiL+GzatIlXX32V5ORktm/fzs8//8yxY8fO2S88PJzWrVvTqVMnunbtymWXXeZBteK1gIW7/2AdgbuAdOC4c+5xM/sn8INzbrSZtQYGAKuBK4FVzrkXL3RMhbtI1jIyMjh27BhHjx5l165dbN26lVWrVjF79mw+//xzwsLCuOmmm+jSpQt169alTp06VKhQATPzunTJZwEN9/ygcBfJnW3btjF58mQmT57Mt99+e3p72bJlqVOnDnXq1OHaMmW4ZcUKri5R4uICv0IFeOMNiIoKfOFLl8Lw4VCqFLz+OpQrF/hzFAEKd5EQd/LkSb788ku2bNnC1q1b2bJly+mvnTt3AnBlyZJ0LF+ejuXK0b5cOcoXK5b1AX/6CVasgLlz4eabA1/wfffB22/DiRO+v0DuvTfw5ygCchruEQVRjIgEXnh4OLVr16Z27bMeKXGOr6tWZW7VqsyrVo2pixczYdcuzIzrr7+eoUOHkpCQcO4V/ZEjcMklkJIS+HB3znfcTp1gwQLfa4V7vtL0AyKh5quvuHzPHh5ITGTWrFl8//33fPzxxwwfPpzNmzfTsWNHWrZsyZw5c84cZx8VBXFxvu6TfKiJb76B9u2hdev8OYecQeEuEmpOTV7Wti0AxYoVo2XLlowYMYIvv/ySF198kT179nDbbbdx3XXXMX36dDIyMn79MytWwOHD+VdT27awaRN8911gzyFnUJ+7SLB6+ml49tlztx88CJGRsHcvZHEz9fjx47z55pskJSWxZcsW6tWrxyOPPMJdl1xCqc6doWJFuFD//MXKXNOnn0Lz5r6bt8WL+z6/5hpITs6y3jybNw9694aTJ/Pn+BerUyf4z39y9UfV5y4S6l55xTfypF27cz9r3/6CQVmsWDF69OjBfffdx7Rp0xg1ahQ9e/akT4kSXFOpEvXLlOG6ihVpVqkS11aoQGREAKLiVE1xcTBs2K9X7jt2+PrhN23yhXx+eOMN3w3ju+/On+NfrGuvzfdT6MpdJBjt3QuVK8Po0fDoo3k+XEZGBh9//DGzZs3iiy++4Isvvjg9zDIiIoKGDRvSuHFjypUrR2RkJJGRkZw8eZKjR49Su3ZtMjIy+OmnnyhZsiSVKlWiSpUq1KhRgypVqhAWlk3v79atUKcOvPgi9OmT57acwzm4/HJo2dI3WifI6cpdJJSduiHp71fPq7CwMNq0aUObNm1Ob9u9ezepqamkpqby6aefMmfOHA4dOsSRI0dO7xMeHs7JC3R1FC9enOrVqxMdHU3p0qWJjo4mNjaWmjVr0rJlS6Kjo+Gqq+DSS3398vkR7qdu5sbHB/7YhZjCXSQ3TpyA48e9O//ixb7RLU2a5NspLr30Uu644w7uuOOOM7Y75zh27BhmRlhYGGlpaRQvXpxSpUpx7NgxvvvuO3bv3k1aWho7duwgLS2NH374gUOHDrFhwwbefPNNAMyMZs2a0bVrVxJbtqRMSopvOGag+90X+mcnD9BfhMFC3TIiF+vgQahVC/ZlOXVSwejQwddXHWT27t3L9u3bWbBgAe+99x4rV66kZLFitDl+nA5AB6AxAR7KV7Gir48/uy6iIKBuGZH88uGHvmDv3x+qVfOujttu8+7ceVCpUiUqVapEixYtGD58OCtWrOCNyZNJnjGDR/fsAaBCVBQta9QgpnJlYipXpkHlyjS89NLcz50TFxcSwX4xdOUucrEeeQSeew4OHMifOViKsN27d7Nw4UIWLFjAqlWr2LJlC8f93V+1a9emd+/e9OjRw9dXX0RpbhmR/NKsmW8M+EcfeV1JyDt+/Dhbt24lNTWVV155hY8//pjixYvTqVMn7rnnHm6++WZKlizpdZkFSuEuwevnn33D4wqjX37xhfujj8ITT3hdTZGzfv16XnrpJaZOncq+ffsoXbo0nTp1olu3brRv356oIvAvKYW7BK8ePWDKFK+ruLDkZEhI8LqKIuvEiRMsWrSIt99+mxkzZnDgwAHAN8KnXr163H777fzhD3+gXAhOK6xwl+DknG/Mc4MG8MADXldzfr/5DdxwQ/49Ki8X5ZdffiE5OZk1a9awbds2VqxYwfr164mKiqJXr14kJibSsGHD7B+mChIKdwlOmzdD3bowYQL88Y9eVyNBau3atTz33HO88cYbHD9+nBIlSlChQoXTXxUrVuSqq64iLi6OuLg4qlWrFjSrWCncJThNmOB7SnHzZt8j6SJ5sHfvXubOncv69ev54Ycf+P777/n+++9JT09n+/btnDhxAoCqVavyu9/9jl69ehETE+Nx1RemcJfg9Pvfw6JFsGuXuj0kX/3888+sXbuWFStWsGjRIt5//32OHz9O06ZN6dOnD/fddx/FT81aWYjkNNxDoxNKQsOp1Xri4xXsku9KlixJs2bN6N+/PzNmzODbb79lzJgxHDlyhMTERGrVqsXYsWM5HOi57QuIwl0Kj6++gm+/LXJzgEjhEB0dzYABA1i3bh0ffPABtWrVYsCAAVxxxRUMHDiQlStX4lVPR27kKNzNrIOZjTezEWb29/N8bmb2sP/rWTObFPhSJeSdtYKQiBfMjBtuuIElS5bw0UcfER8fz/jx44mLi+Pqq69mxIgRbNmy5aKPe/LkSdavX89jjz3GhAkT8qHyM2Xb525mUcA6IMY5d8zMpgPjnXMLM+3T3X+sKf73DZ1z6y50XPW5yzl69oQ5cy64gpCIFw4cOMCMGTN44403WLx4Mc45rrvuOlq1akXDhg0pXbo06enpHDhwgIyMDDIyMtizZw8//PADhw8fJj09nfXr13PkyBHCwsLo27cvL7zwQq5qCdgNVTNLAIY65xL87wcB1ZxzgzLtMx+YD2QAVYCXnXNfXui4noX7xo2+9SFjs/1vI3k1e/bFPWn6zDPQogVMn55/NYnk0a5du3j77beZPn06n332WZZ98hUqVCA6OppSpUpRrlw56tevT8OGDbnpppu49NJLc33+QM4KWQk4lOn9Qf+2zGoAZZxz/zCzOsB8M7vGOXfGLP5m1hvoDXD55Zfn4NT5oHdv+Ppr39JeujrMPwcOQOfOcGrh5Zzq1Cl/6hEJkKpVqzJw4EAGDhxIRkYGO3bs4OjRo0RHR1O2bFnCw8NPz3XvpZyE+16gdKb3ZfzbMjsIpAI457aYWRmgOrAj807OuQnABPBdueeu5Dw4cgRSU32LLOzYAVdeWeAlFBkffeQL9nnzoFWrnP2ZsDDfmqAiQSIsLIyaNWt6XcZ55STclwE1zKyEc+4Y0AoYb2blgRPOuYPAQqAmgD/Yw4E9+VRz7p0KdvAtU6Zwzz8pKb6V7du29a16LyIFKtt/NzjnjgB9gefNbBSwzn8zdTDwoH+3p4DGZjYUGAP0cM79nE81515Kiu/qsEyZX0dmSP5ISfHNnqhgF/FEjlZics4tABacte2RTK9/BPJhZdsAS0mBxo2henWFe35YswYmTvR1x6xeDYMHe12RSJFVdJbZO3YMli+Hvn194T5rlm9FdC+XSQs1SUkwcyaULw+XXQZdunhdkUiRVXSeUP30U98iEG3b/vqQjK7eA8c5332M3/3OtxBxWhpce63XVYkUWUUn3Jcu9X1v0wYaNYJLLvl1m+Td5s2+h4/0dKlIoVB0umVSUnwLQJQv73vfurWu3ANJUweIFCqhE+779vkWd8hqBrelS89c/KFtW9+j7h07nvswU7lyMGmSxlyfz6uvwtSp527fvNm3gtJVVxV8TSJyjtAJ99mzfTfz4uIg4jzNatrUtzbnKXff7XvA5uy/DI4ehQULfPOK//a3+VtzMBo1Cn76CWrVOnP7ZZdBt2566lekkAidcE9Jgeho34NKOQmYK67wLQpxtp9/hrJlfcdTuJ/pm2/gyy9hzBgYMMDrakTkAkLnhmqgFnkoWdL38I1utp7r1H+T+Hhv6xCRbIVGuKel+b4CFTpt2/oewjl4MDDHCxUpKb5RRo0aeV2JiGQjNLplTl1RBmqkRtu2MHIkXHMNlCgRmGOGgt27ISEBwsO9rkREshEa4Z6S4hvh0qBBYI4XH+/rU/7++8AcL1SY+aZMFpFCL3TCvU3cqttHAAAJWklEQVQb36RggVCsmO+moYhIkAr+Pvddu2DbNj08IyKSSfCHu56MFBE5R/CH+9KlULq0RnCIiGQS/OGekuKbJ+Z8T6WKiBRRwR3ue/fCxo3qkhEROUtwh3ugx7eLiISI4A73lBTfzI1NmnhdiYhIoRLc4b5sGTRv7huXLiIipwV3uO/a5ZvdUUREzpCjISZm1gG4E9gLOOfc41ns93vgdaC0c+6ngFV5Ps75FuiIjs7X04iIBKNsw93MooAXgRjn3DEzm25mCc65hWftdw1QL5/qPNePP8Lx4wp3EZHzyEm3TAsgzTl3zP/+Y+DWzDv4/wJ4BDjvFX2+SE/3fVe4i4icIyfhXgk4lOn9Qf+2zJ4ARjrnfrnQgcyst5mtNLOV6afCObcU7iIiWcpJuO8FSmd6X8a/DQAzqw6UA7qa2WD/5kFmFnv2gZxzE5xzsc652Oi8hrLCXUQkSzm5oboMqGFmJfxdM62A8WZWHjjhnNsJ9Dy1s5k9CTyb7zdUFe4iIlnK9srdOXcE6As8b2ajgHX+m6mDgQdP7Wdm0WY2zP/2ETO7LD8KPm3fPt/3ihXz9TQiIsEoR0MhnXMLgAVnbXvkrPfpwCj/V/5LT4eoKN+XiIicIXgfYkpPV5eMiEgWFO4iIiFI4S4iEoIU7iIiIUjhLiISgoIz3H/8EY4ehcqVva5ERKRQCs5wT0vzfdd0vyIi5xWc4b5jh+97jRqeliEiUlgFZ7jryl1E5IKCM9x37IDISN1QFRHJQnCGe1qar0vGzOtKREQKpeAM9x071N8uInIBwRnuaWnqbxcRuYDgC/fDh33T/erKXUQkS8EX7hopIyKSreAL97AwuOsuiInxuhIRkUIrR4t1FCp168I773hdhYhIoRZ8V+4iIpIthbuISAhSuIuIhCCFu4hICMrRDVUz6wDcCewFnHPu8bM+fxSoAuwBmgDDnXObAlyriIjkULbhbmZRwItAjHPumJlNN7ME59zCTLv9BhjknHNm1g34F3B7/pQsIiLZyUm3TAsgzTl3zP/+Y+DWzDs45/7mnHOZjvlT4EoUEZGLlZNumUrAoUzvD/q3ncPMigM9gH55L01ERHIrJ+G+Fyid6X0Z/7Yz+IP9P8Bjzrnt5zuQmfUGevvf/mRmmy+u3DNUBPbl4c8Hk6LUVlB7Q1lRaivkT3tzNLGW/dqbksUOvj73dWTqcwfGA2uAE865g2YW6d/2tHNuvZl1cc5Nz1v92RRuttI5F5uf5ygsilJbQe0NZUWpreBte7O9cnfOHTGzvsDzZpYOrHPOLTSzfwI/AKOBN4D6wJXmW0CjFJCv4S4iIlnL0VBI59wCYMFZ2x7J9PrOANclIiJ5EMwPMU3wuoACVJTaCmpvKCtKbQUP25ttn7uIiASfYL5yFxGRLATdfO7ZTYUQCsxsOfCz/+1J51yCmZXHd/P6S6A2MNQ5951XNeaFmVUBRgGNnHNx/m0lgaeBb/G1b7Rzbov/s/uAa4GTwHbn3EueFJ4LWbS1J/AAv/6MX3HO/df/WTC3tRa+tq4GqgHfO+f+caHfXTP7K77h1eWA/znnZntSfC5coL0jgOsz7fqE/75lwbbXORc0X0AUsA0o4X8/HUjwuq58aOeI82x7Eejqf3078F+v68xD++7yt2Flpm2DgUf8rxsAH/pfVwM+49cuxBVAba/bkMe29gSuOM++wd7WOKBTpvcb8M01dd7fXaAZMNf/uhiwFSjrdTsC0N4RWexfoO0Ntm6ZbKdCCBENzOxRMxthZqfadyuwzP86qNvtnJvGmU89Q6b2Oec+BxqZWRngRmCV8/8f4d/n5oKqNa+yaCtAfzP7i5kN91/ZQvC3dYVzblamTWHAYbL+3b2NX3/mx4GNQHzBVJt3F2gvZvaY/+f7qP9ZISjg9gZbt0yOp0IIck855z41s3BgqZkd4sy2HwTKmVmEc+6EZ1UGVlY/21D8macAc5xz6WZ2C/AOkEAItdXMOgMfOOc2mdl5f3fxtW1jpj8WKu19B9jhnDtsZg8C44BECri9wXblnqOpEIKdc+5T//eTwIdAO85sexlgfwgFO2T9sw25n7lz7ivnXLr/7SKgrf8v8pBoq5m1w/c7O9C/Kavf3ZBsr3NuvXPusP/jRUB7/+sCbW+whfsyoIaZlfC/bwXM8bCegDOzumaWmGlTbXz3Gebg65aCEGw3mdpnZg2Atc65g8AHQBPzP/rs32eeNyUGhpk96b9yBd/P9yv/X+RB31Z/N+KNwJ+AKmbWgqx/d9/n1595BFAPWFqgBefR+dprZv/KtMup/3+hgNsbdOPczawjvptU6cBxF2KjZcysKvBvfHfgy+C78TIIKAs8BaQBtYDBLnhHy7QF/gDchG+yuWf8Hz0N7AauApLcmaNlYvGNINnigmsEyfna2hvfdB1f4bt5PNY5t9y/fzC3tQm+LqeV/k2l8P0uzyaL313/6JFy/q95LrhGy2TV3qvxDf7Yi+/nOzzT73KBtTfowl1ERLIXbN0yIiKSAwp3EZEQpHAXEQlBCncRkRCkcBcRCUEKd5EcMrNbzewrM7vC61pEsqNwF8kh59wcfGO1RQq9YJtbRiRbZvYPfL/bJ/HNabIHeB5Iwvf4dyPgT865r8ysFdAD31OEdYFhzrld/u09gS34Zv97+tS0EEBXM6sJXAPc7nyLxD/uP+cxoLhzbljBtFbk/BTuElLM7EaguXPuBv/7JcAA4AAwwzm3zcy6Af80s67A28C1/km8ugFPm9nv/dubOOe+M7P6+J4+PGWNc+6fZvYC0BHf1NO9gfbOuY1m1rKAmiuSJYW7hJqGQJSZDfa/3wlE+19/6f++DYgBKgJlMk3itQ3fVf2p7d8BOOe+OOscp+YK2cevE0HdAySZWWV8/0r4JGAtEskFhbuEmrVAC+fcaAAza8+vYVzT/7oOvoUV9gE/mlkl59xefJM8fXb2djNrCPzGOXcqsM83Z0dp51xn//S2a4G38ql9IjmiuWUk5JjZMHzdKCeAkvhWedqOb6m36viWsXvIObfd37fey//51fgmtdqdaftWoCowDN9KOhOA/wKTgZeB/fiWzHsR32RvkcAR51xSgTRWJAsKdykSzGyHc+4Kr+sQKSgaCikhz3+D9BL/qjgiRYKu3EVEQpCu3EVEQpDCXUQkBCncRURCkMJdRCQEKdxFREKQwl1EJAT9f1jPKilcgY9SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a149d1eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 学習の進み具合を可視化\n",
    "\n",
    "val_acc = hist.history['val_acc']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "plt.rc('font', family='serif')\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(val_acc)), val_acc, label='val_acc', color='red')\n",
    "plt.plot(range(len(val_loss)), val_loss, label='val_loss', color='black')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 96us/step\n",
      "[0.62265030516160502, 0.7567567567567568]\n"
     ]
    }
   ],
   "source": [
    "# 予測精度の評価\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print(loss_and_metrics) # 1番目：誤差関数の値、2番目：予測精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 4 4 4 1 4 0 4]\n"
     ]
    }
   ],
   "source": [
    "# テスト用データを使用して予測する\n",
    "test_size = 10\n",
    "X_predict = np.argmax(model.predict(X_test[0:test_size]),axis=1)\n",
    "print(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 4 3 2 4 0 2]\n"
     ]
    }
   ],
   "source": [
    "# テスト用データの正解　0：資産　1：負債　2：純資産　3：収益　4：費用\n",
    "print(np.argmax(Y_test[0:test_size],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False,  True, False, False,  True,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 予測が正しかったか判定する\n",
    "X_predict == np.argmax(Y_test[0:test_size],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1067pt\" viewBox=\"0.00 0.00 273.98 1067.00\" width=\"274pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1063)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-1063 269.9766,-1063 269.9766,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 111998194800 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>111998194800</title>\n",
       "<polygon fill=\"none\" points=\"49.3691,-1022.5 49.3691,-1058.5 216.6074,-1058.5 216.6074,-1022.5 49.3691,-1022.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-1036.3\">dense_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 111998194464 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>111998194464</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-949.5 80.8623,-985.5 185.1143,-985.5 185.1143,-949.5 80.8623,-949.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-963.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 111998194800&#45;&gt;111998194464 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>111998194800-&gt;111998194464</title>\n",
       "<path d=\"M132.9883,-1022.4551C132.9883,-1014.3828 132.9883,-1004.6764 132.9883,-995.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-995.5903 132.9883,-985.5904 129.4884,-995.5904 136.4884,-995.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111998257304 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>111998257304</title>\n",
       "<polygon fill=\"none\" points=\"0,-876.5 0,-912.5 265.9766,-912.5 265.9766,-876.5 0,-876.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-890.3\">batch_normalization_1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 111998194464&#45;&gt;111998257304 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>111998194464-&gt;111998257304</title>\n",
       "<path d=\"M132.9883,-949.4551C132.9883,-941.3828 132.9883,-931.6764 132.9883,-922.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-922.5903 132.9883,-912.5904 129.4884,-922.5904 136.4884,-922.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111998257360 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>111998257360</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-803.5 57.5278,-839.5 208.4487,-839.5 208.4487,-803.5 57.5278,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-817.3\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 111998257304&#45;&gt;111998257360 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>111998257304-&gt;111998257360</title>\n",
       "<path d=\"M132.9883,-876.4551C132.9883,-868.3828 132.9883,-858.6764 132.9883,-849.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-849.5903 132.9883,-839.5904 129.4884,-849.5904 136.4884,-849.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111998769584 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>111998769584</title>\n",
       "<polygon fill=\"none\" points=\"69.1865,-730.5 69.1865,-766.5 196.79,-766.5 196.79,-730.5 69.1865,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-744.3\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 111998257360&#45;&gt;111998769584 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>111998257360-&gt;111998769584</title>\n",
       "<path d=\"M132.9883,-803.4551C132.9883,-795.3828 132.9883,-785.6764 132.9883,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-776.5903 132.9883,-766.5904 129.4884,-776.5904 136.4884,-776.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111998195528 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>111998195528</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-657.5 80.8623,-693.5 185.1143,-693.5 185.1143,-657.5 80.8623,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-671.3\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 111998769584&#45;&gt;111998195528 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>111998769584-&gt;111998195528</title>\n",
       "<path d=\"M132.9883,-730.4551C132.9883,-722.3828 132.9883,-712.6764 132.9883,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-703.5903 132.9883,-693.5904 129.4884,-703.5904 136.4884,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111998889208 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>111998889208</title>\n",
       "<polygon fill=\"none\" points=\"0,-584.5 0,-620.5 265.9766,-620.5 265.9766,-584.5 0,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-598.3\">batch_normalization_2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 111998195528&#45;&gt;111998889208 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>111998195528-&gt;111998889208</title>\n",
       "<path d=\"M132.9883,-657.4551C132.9883,-649.3828 132.9883,-639.6764 132.9883,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-630.5903 132.9883,-620.5904 129.4884,-630.5904 136.4884,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111998530896 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>111998530896</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-511.5 57.5278,-547.5 208.4487,-547.5 208.4487,-511.5 57.5278,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-525.3\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 111998889208&#45;&gt;111998530896 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>111998889208-&gt;111998530896</title>\n",
       "<path d=\"M132.9883,-584.4551C132.9883,-576.3828 132.9883,-566.6764 132.9883,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-557.5903 132.9883,-547.5904 129.4884,-557.5904 136.4884,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111999012088 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>111999012088</title>\n",
       "<polygon fill=\"none\" points=\"69.1865,-438.5 69.1865,-474.5 196.79,-474.5 196.79,-438.5 69.1865,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-452.3\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 111998530896&#45;&gt;111999012088 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>111998530896-&gt;111999012088</title>\n",
       "<path d=\"M132.9883,-511.4551C132.9883,-503.3828 132.9883,-493.6764 132.9883,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-484.5903 132.9883,-474.5904 129.4884,-484.5904 136.4884,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111999011080 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>111999011080</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-365.5 80.8623,-401.5 185.1143,-401.5 185.1143,-365.5 80.8623,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-379.3\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 111999012088&#45;&gt;111999011080 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>111999012088-&gt;111999011080</title>\n",
       "<path d=\"M132.9883,-438.4551C132.9883,-430.3828 132.9883,-420.6764 132.9883,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-411.5903 132.9883,-401.5904 129.4884,-411.5904 136.4884,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111999266040 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>111999266040</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 265.9766,-328.5 265.9766,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-306.3\">batch_normalization_3: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 111999011080&#45;&gt;111999266040 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>111999011080-&gt;111999266040</title>\n",
       "<path d=\"M132.9883,-365.4551C132.9883,-357.3828 132.9883,-347.6764 132.9883,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-338.5903 132.9883,-328.5904 129.4884,-338.5904 136.4884,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111999794984 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>111999794984</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-219.5 57.5278,-255.5 208.4487,-255.5 208.4487,-219.5 57.5278,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-233.3\">activation_3: Activation</text>\n",
       "</g>\n",
       "<!-- 111999266040&#45;&gt;111999794984 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>111999266040-&gt;111999794984</title>\n",
       "<path d=\"M132.9883,-292.4551C132.9883,-284.3828 132.9883,-274.6764 132.9883,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-265.5903 132.9883,-255.5904 129.4884,-265.5904 136.4884,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111999733600 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>111999733600</title>\n",
       "<polygon fill=\"none\" points=\"69.1865,-146.5 69.1865,-182.5 196.79,-182.5 196.79,-146.5 69.1865,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-160.3\">dropout_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 111999794984&#45;&gt;111999733600 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>111999794984-&gt;111999733600</title>\n",
       "<path d=\"M132.9883,-219.4551C132.9883,-211.3828 132.9883,-201.6764 132.9883,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-192.5903 132.9883,-182.5904 129.4884,-192.5904 136.4884,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111998194240 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>111998194240</title>\n",
       "<polygon fill=\"none\" points=\"80.8623,-73.5 80.8623,-109.5 185.1143,-109.5 185.1143,-73.5 80.8623,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-87.3\">dense_4: Dense</text>\n",
       "</g>\n",
       "<!-- 111999733600&#45;&gt;111998194240 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>111999733600-&gt;111998194240</title>\n",
       "<path d=\"M132.9883,-146.4551C132.9883,-138.3828 132.9883,-128.6764 132.9883,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-119.5903 132.9883,-109.5904 129.4884,-119.5904 136.4884,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 111998194408 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>111998194408</title>\n",
       "<polygon fill=\"none\" points=\"57.5278,-.5 57.5278,-36.5 208.4487,-36.5 208.4487,-.5 57.5278,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.9883\" y=\"-14.3\">activation_4: Activation</text>\n",
       "</g>\n",
       "<!-- 111998194240&#45;&gt;111998194408 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>111998194240-&gt;111998194408</title>\n",
       "<path d=\"M132.9883,-73.4551C132.9883,-65.3828 132.9883,-55.6764 132.9883,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"136.4884,-46.5903 132.9883,-36.5904 129.4884,-46.5904 136.4884,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
